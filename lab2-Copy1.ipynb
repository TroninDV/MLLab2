{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nssWjlpnJ5rU"
   },
   "source": [
    "# Лабораторная работа 2. Вероятностные модели.\n",
    "\n",
    "Результат лабораторной работы − отчет. Мы предпочитаем принимать отчеты в формате ноутбуков IPython (ipynb-файл). Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете также должен быть код, однако чем меньше кода, тем лучше всем: нам − меньше проверять, вам — проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.\n",
    "\n",
    "Мы уверены, что выполнение лабораторных работ занимает значительное время, поэтому не рекомендуем оставлять их на последний вечер перед сдачей.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "* Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи)\n",
    "* Максимально допустимая оценка за работу — 15 баллов. Также в результате выполнения заданий у вас получится решение [задачи конкурса](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2021), которое после небольшой доработки принесёт ещё 5 баллов за пробитие Medium Baseline.\n",
    "* Сдавать задание после указанного срока сдачи нельзя\n",
    "* «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают карму (подробнее о плагиате см. на странице курса)\n",
    "* Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник)\n",
    "* Не оцениваются задания с удалёнными формулировками\n",
    "* Не оценивается лабораторная работа целиком, если она была выложена в открытый источник\n",
    "\n",
    "Обратите внимание, что мы не ставим оценку за просто написанный код, корректная работоспособность которого не подтверждена экспериментами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e83Dpd2Q4s3"
   },
   "source": [
    "Цель этой лабораторной работы – научиться строить вероятностные модели и оптимизировать их параметры на примере задачи оценки риска заболевания сахарным диабетом. На задачу оценки риска болезни мы посмотрим со стороны страховой компании.\n",
    "\n",
    "Если человек перестанет проходить обследования, и страховая так и не узнает, развился ли у него диабет, то и расходов, связанных с его заболеванием не будет, т.е. можно считать, что такой человек остался здоров. То же касается людей, у которых заболевание впервые обнаружат более чем через 5 лет.\n",
    "\n",
    "Чтобы рассчитать математическое ожидание затрат на лечение клиента, страховая хочет получить в качестве результата работы модели непосредственно вероятность того, что у человека, не страдающего от заболевания, оно разовьётся в течение 5 лет. Поэтому в качестве метрики качества была выбрана бинарная кросс-энтропия (она же logloss) между предсказанными вероятностями и истинными метками классов:\n",
    "\n",
    "\n",
    "$$\\text{crossentropy}(y, p) = -\\frac{1}{N}\\sum_{i=1}^N\\left[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxf4DgkuKRrq"
   },
   "source": [
    "**Задание 1** Препроцессинг (1 балл) \n",
    "\n",
    "- Прочитайте описание [набора данных и задачи конкурса 2](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2021).\n",
    "- Загрузите обучающий набор данных (X_train.csv, y_train.csv).\n",
    "- Обратите внимание, что часть информации о клиентах неизвестна на момент заключения договора. Соответствующие признаки отсутствуют в X_test.csv.\n",
    "- Заполните пропуски в данных. Для этого могут пригодиться методы из sklearn.impute или pandas.DataFrame.fillna.\n",
    "- По желанию используйте любой препроцессинг данных, добавляйте новые признаки и т.п. ваша задача — добиться сходимости и высокого качества полученных моделей.\n",
    "- Разбейте обучающую выборку на lab_train и lab_test, которые будете использовать для оценки всех построенных моделей в лабораторной работе. При желании использовать для оценки качества кросс-валидацию необходимо проконтролировать, чтобы для всех моделей использовались одни и те же разбиения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2-Hv-63eQZvv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>site</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>diastolic blood pressure</th>\n",
       "      <th>Fasting plasma glucose</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>Aspartate transaminase</th>\n",
       "      <th>Blood urea nitrogen</th>\n",
       "      <th>Creatinine clearance rate</th>\n",
       "      <th>Fasting plasma glucose (final visit)</th>\n",
       "      <th>Diabetes diagnosed during followup</th>\n",
       "      <th>censor of diabetes at followup</th>\n",
       "      <th>year of followup</th>\n",
       "      <th>smoking status</th>\n",
       "      <th>drinking status</th>\n",
       "      <th>family history of diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>163.5</td>\n",
       "      <td>64.1</td>\n",
       "      <td>24.00</td>\n",
       "      <td>106.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.83</td>\n",
       "      <td>3.84</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.01</td>\n",
       "      <td>74.8</td>\n",
       "      <td>5.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.699521</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>156.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>28.10</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>4.12</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3</td>\n",
       "      <td>3.19</td>\n",
       "      <td>54.7</td>\n",
       "      <td>4.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.187543</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>152.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>20.30</td>\n",
       "      <td>111.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>4.30</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.042437</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>177.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.71</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>26.5</td>\n",
       "      <td>5.79</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.028747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>178.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.50</td>\n",
       "      <td>124.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>5.02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.68</td>\n",
       "      <td>76.9</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4.950034</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>161.5</td>\n",
       "      <td>57.0</td>\n",
       "      <td>21.90</td>\n",
       "      <td>121.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>5.54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.40</td>\n",
       "      <td>75.9</td>\n",
       "      <td>5.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4.177960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>177.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>26.30</td>\n",
       "      <td>123.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4.76</td>\n",
       "      <td>5.70</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.78</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.073238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>173.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>20.00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>3.57</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.20</td>\n",
       "      <td>82.6</td>\n",
       "      <td>4.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.017796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>187.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>27.50</td>\n",
       "      <td>147.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.22</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23</td>\n",
       "      <td>73.5</td>\n",
       "      <td>5.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.735113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>158.5</td>\n",
       "      <td>67.1</td>\n",
       "      <td>26.70</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>5.29</td>\n",
       "      <td>...</td>\n",
       "      <td>22.9</td>\n",
       "      <td>4.57</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.050650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Gender  site  height  weight  Body Mass Index  \\\n",
       "0       53       1    13   163.5    64.1            24.00   \n",
       "1       26       2     3   156.0    68.5            28.10   \n",
       "2       40       2     5   152.0    47.0            20.30   \n",
       "3       34       1     4   177.0    68.0            21.71   \n",
       "4       34       1     3   178.0    68.0            21.50   \n",
       "...    ...     ...   ...     ...     ...              ...   \n",
       "99995   72       1     7   161.5    57.0            21.90   \n",
       "99996   51       1     5   177.5    83.0            26.30   \n",
       "99997   34       1     7   173.0    59.9            20.00   \n",
       "99998   32       1     2   187.0    96.0            27.50   \n",
       "99999   56       2     4   158.5    67.1            26.70   \n",
       "\n",
       "       Systolic blood pressure  diastolic blood pressure  \\\n",
       "0                        106.0                      68.0   \n",
       "1                         98.0                      68.0   \n",
       "2                        111.0                      63.0   \n",
       "3                        133.0                      81.0   \n",
       "4                        124.0                      84.0   \n",
       "...                        ...                       ...   \n",
       "99995                    121.0                      87.0   \n",
       "99996                    123.0                      79.0   \n",
       "99997                     89.0                      55.0   \n",
       "99998                    147.0                     104.0   \n",
       "99999                    133.0                      81.0   \n",
       "\n",
       "       Fasting plasma glucose  Cholesterol  ...  Aspartate transaminase  \\\n",
       "0                        4.83         3.84  ...                     NaN   \n",
       "1                        5.12         4.12  ...                    15.3   \n",
       "2                        5.62         4.30  ...                    17.0   \n",
       "3                        4.84         4.35  ...                    26.5   \n",
       "4                        3.73         5.02  ...                     NaN   \n",
       "...                       ...          ...  ...                     ...   \n",
       "99995                    5.90         5.54  ...                     NaN   \n",
       "99996                    4.76         5.70  ...                     NaN   \n",
       "99997                    5.20         3.57  ...                     NaN   \n",
       "99998                    4.77         4.22  ...                     NaN   \n",
       "99999                    5.13         5.29  ...                    22.9   \n",
       "\n",
       "       Blood urea nitrogen  Creatinine clearance rate  \\\n",
       "0                     5.01                       74.8   \n",
       "1                     3.19                       54.7   \n",
       "2                     4.38                       66.0   \n",
       "3                     5.79                       78.9   \n",
       "4                     3.68                       76.9   \n",
       "...                    ...                        ...   \n",
       "99995                 5.40                       75.9   \n",
       "99996                 4.78                       68.0   \n",
       "99997                 4.20                       82.6   \n",
       "99998                 3.23                       73.5   \n",
       "99999                 4.57                       52.0   \n",
       "\n",
       "       Fasting plasma glucose (final visit)  \\\n",
       "0                                      5.23   \n",
       "1                                      4.33   \n",
       "2                                      4.70   \n",
       "3                                      5.69   \n",
       "4                                      5.00   \n",
       "...                                     ...   \n",
       "99995                                  5.22   \n",
       "99996                                  6.30   \n",
       "99997                                  4.29   \n",
       "99998                                  5.10   \n",
       "99999                                  5.50   \n",
       "\n",
       "       Diabetes diagnosed during followup  censor of diabetes at followup  \\\n",
       "0                                     NaN                               0   \n",
       "1                                     NaN                               0   \n",
       "2                                     NaN                               0   \n",
       "3                                     NaN                               0   \n",
       "4                                     NaN                               0   \n",
       "...                                   ...                             ...   \n",
       "99995                                 NaN                               0   \n",
       "99996                                 NaN                               0   \n",
       "99997                                 NaN                               0   \n",
       "99998                                 NaN                               0   \n",
       "99999                                 NaN                               0   \n",
       "\n",
       "       year of followup  smoking status  drinking status  \\\n",
       "0              2.699521             3.0              3.0   \n",
       "1              2.187543             3.0              3.0   \n",
       "2              2.042437             3.0              3.0   \n",
       "3              2.028747             NaN              NaN   \n",
       "4              4.950034             3.0              3.0   \n",
       "...                 ...             ...              ...   \n",
       "99995          4.177960             NaN              NaN   \n",
       "99996          5.073238             NaN              NaN   \n",
       "99997          2.017796             NaN              NaN   \n",
       "99998          2.735113             NaN              NaN   \n",
       "99999          2.050650             NaN              NaN   \n",
       "\n",
       "       family history of diabetes  \n",
       "0                               0  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               0  \n",
       "...                           ...  \n",
       "99995                           0  \n",
       "99996                           0  \n",
       "99997                           0  \n",
       "99998                           0  \n",
       "99999                           0  \n",
       "\n",
       "[100000 rows x 24 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_train_col = ['year of followup', 'Fasting plasma glucose (final visit)', 'censor of diabetes at followup', \n",
    "                 'Diabetes diagnosed during followup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(only_train_col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns=['Gender', 'site', 'smoking status', 'drinking status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                      0.00000\n",
       "height                                   0.00002\n",
       "weight                                   0.00000\n",
       "Body Mass Index                          0.00000\n",
       "Systolic blood pressure                  0.00011\n",
       "diastolic blood pressure                 0.00012\n",
       "Fasting plasma glucose                   0.00000\n",
       "Cholesterol                              0.02257\n",
       "Triglyceride                             0.02277\n",
       "High-density lipoprotein cholesterol     0.44444\n",
       "Low-density lipoprotein cholesterol      0.43884\n",
       "Alanine aminotransferase                 0.00837\n",
       "Aspartate transaminase                   0.58247\n",
       "Blood urea nitrogen                      0.10080\n",
       "Creatinine clearance rate                0.05200\n",
       "family history of diabetes               0.00000\n",
       "Gender_1                                 0.00000\n",
       "Gender_2                                 0.00000\n",
       "site_0                                   0.00000\n",
       "site_2                                   0.00000\n",
       "site_3                                   0.00000\n",
       "site_4                                   0.00000\n",
       "site_5                                   0.00000\n",
       "site_6                                   0.00000\n",
       "site_7                                   0.00000\n",
       "site_8                                   0.00000\n",
       "site_9                                   0.00000\n",
       "site_10                                  0.00000\n",
       "site_11                                  0.00000\n",
       "site_12                                  0.00000\n",
       "site_13                                  0.00000\n",
       "site_14                                  0.00000\n",
       "site_16                                  0.00000\n",
       "smoking status_1.0                       0.00000\n",
       "smoking status_2.0                       0.00000\n",
       "smoking status_3.0                       0.00000\n",
       "drinking status_1.0                      0.00000\n",
       "drinking status_2.0                      0.00000\n",
       "drinking status_3.0                      0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>diastolic blood pressure</th>\n",
       "      <th>Fasting plasma glucose</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Triglyceride</th>\n",
       "      <th>High-density lipoprotein cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>site_12</th>\n",
       "      <th>site_13</th>\n",
       "      <th>site_14</th>\n",
       "      <th>site_16</th>\n",
       "      <th>smoking status_1.0</th>\n",
       "      <th>smoking status_2.0</th>\n",
       "      <th>smoking status_3.0</th>\n",
       "      <th>drinking status_1.0</th>\n",
       "      <th>drinking status_2.0</th>\n",
       "      <th>drinking status_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>163.5</td>\n",
       "      <td>64.1</td>\n",
       "      <td>24.00</td>\n",
       "      <td>106.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.83</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>156.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>28.10</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>152.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>20.30</td>\n",
       "      <td>111.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>177.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.71</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>178.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.50</td>\n",
       "      <td>124.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>5.02</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>72</td>\n",
       "      <td>161.5</td>\n",
       "      <td>57.0</td>\n",
       "      <td>21.90</td>\n",
       "      <td>121.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>5.54</td>\n",
       "      <td>1.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>51</td>\n",
       "      <td>177.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>26.30</td>\n",
       "      <td>123.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4.76</td>\n",
       "      <td>5.70</td>\n",
       "      <td>2.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>34</td>\n",
       "      <td>173.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>20.00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>32</td>\n",
       "      <td>187.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>27.50</td>\n",
       "      <td>147.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.22</td>\n",
       "      <td>1.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>56</td>\n",
       "      <td>158.5</td>\n",
       "      <td>67.1</td>\n",
       "      <td>26.70</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  height  weight  Body Mass Index  Systolic blood pressure  \\\n",
       "0       53   163.5    64.1            24.00                    106.0   \n",
       "1       26   156.0    68.5            28.10                     98.0   \n",
       "2       40   152.0    47.0            20.30                    111.0   \n",
       "3       34   177.0    68.0            21.71                    133.0   \n",
       "4       34   178.0    68.0            21.50                    124.0   \n",
       "...    ...     ...     ...              ...                      ...   \n",
       "99995   72   161.5    57.0            21.90                    121.0   \n",
       "99996   51   177.5    83.0            26.30                    123.0   \n",
       "99997   34   173.0    59.9            20.00                     89.0   \n",
       "99998   32   187.0    96.0            27.50                    147.0   \n",
       "99999   56   158.5    67.1            26.70                    133.0   \n",
       "\n",
       "       diastolic blood pressure  Fasting plasma glucose  Cholesterol  \\\n",
       "0                          68.0                    4.83         3.84   \n",
       "1                          68.0                    5.12         4.12   \n",
       "2                          63.0                    5.62         4.30   \n",
       "3                          81.0                    4.84         4.35   \n",
       "4                          84.0                    3.73         5.02   \n",
       "...                         ...                     ...          ...   \n",
       "99995                      87.0                    5.90         5.54   \n",
       "99996                      79.0                    4.76         5.70   \n",
       "99997                      55.0                    5.20         3.57   \n",
       "99998                     104.0                    4.77         4.22   \n",
       "99999                      81.0                    5.13         5.29   \n",
       "\n",
       "       Triglyceride  High-density lipoprotein cholesterol   ...  site_12  \\\n",
       "0              1.35                                   1.16  ...        0   \n",
       "1              0.72                                    NaN  ...        0   \n",
       "2              0.70                                   1.32  ...        0   \n",
       "3              1.39                                    NaN  ...        0   \n",
       "4              1.12                                    NaN  ...        0   \n",
       "...             ...                                    ...  ...      ...   \n",
       "99995          1.17                                    NaN  ...        0   \n",
       "99996          2.15                                    NaN  ...        0   \n",
       "99997          0.44                                    NaN  ...        0   \n",
       "99998          1.02                                    NaN  ...        0   \n",
       "99999          2.38                                   1.31  ...        0   \n",
       "\n",
       "       site_13  site_14  site_16  smoking status_1.0  smoking status_2.0  \\\n",
       "0            1        0        0                   0                   0   \n",
       "1            0        0        0                   0                   0   \n",
       "2            0        0        0                   0                   0   \n",
       "3            0        0        0                   0                   0   \n",
       "4            0        0        0                   0                   0   \n",
       "...        ...      ...      ...                 ...                 ...   \n",
       "99995        0        0        0                   0                   0   \n",
       "99996        0        0        0                   0                   0   \n",
       "99997        0        0        0                   0                   0   \n",
       "99998        0        0        0                   0                   0   \n",
       "99999        0        0        0                   0                   0   \n",
       "\n",
       "       smoking status_3.0  drinking status_1.0  drinking status_2.0  \\\n",
       "0                       1                    0                    0   \n",
       "1                       1                    0                    0   \n",
       "2                       1                    0                    0   \n",
       "3                       0                    0                    0   \n",
       "4                       1                    0                    0   \n",
       "...                   ...                  ...                  ...   \n",
       "99995                   0                    0                    0   \n",
       "99996                   0                    0                    0   \n",
       "99997                   0                    0                    0   \n",
       "99998                   0                    0                    0   \n",
       "99999                   0                    0                    0   \n",
       "\n",
       "       drinking status_3.0  \n",
       "0                        1  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                        0  \n",
       "4                        1  \n",
       "...                    ...  \n",
       "99995                    0  \n",
       "99996                    0  \n",
       "99997                    0  \n",
       "99998                    0  \n",
       "99999                    0  \n",
       "\n",
       "[100000 rows x 39 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nan = SimpleImputer(strategy='median')\n",
    "X_train = pd.DataFrame(replace_nan.fit_transform(X_train), columns=X_train.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.loc[:, :'Creatinine clearance rate'])\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled.loc[:, :'Creatinine clearance rate'] = \\\n",
    "    scaler.transform(X_train.loc[:, :'Creatinine clearance rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>diastolic blood pressure</th>\n",
       "      <th>Fasting plasma glucose</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Triglyceride</th>\n",
       "      <th>High-density lipoprotein cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>site_12</th>\n",
       "      <th>site_13</th>\n",
       "      <th>site_14</th>\n",
       "      <th>site_16</th>\n",
       "      <th>smoking status_1.0</th>\n",
       "      <th>smoking status_2.0</th>\n",
       "      <th>smoking status_3.0</th>\n",
       "      <th>drinking status_1.0</th>\n",
       "      <th>drinking status_2.0</th>\n",
       "      <th>drinking status_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.861064</td>\n",
       "      <td>-0.357051</td>\n",
       "      <td>-0.050174</td>\n",
       "      <td>0.228036</td>\n",
       "      <td>-0.800849</td>\n",
       "      <td>-0.572614</td>\n",
       "      <td>-0.139734</td>\n",
       "      <td>-0.969961</td>\n",
       "      <td>0.016151</td>\n",
       "      <td>-0.860153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.272461</td>\n",
       "      <td>-1.256006</td>\n",
       "      <td>0.309383</td>\n",
       "      <td>1.452767</td>\n",
       "      <td>-1.290199</td>\n",
       "      <td>-0.572614</td>\n",
       "      <td>0.334063</td>\n",
       "      <td>-0.656250</td>\n",
       "      <td>-0.595753</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.166189</td>\n",
       "      <td>-1.735448</td>\n",
       "      <td>-1.447544</td>\n",
       "      <td>-0.877208</td>\n",
       "      <td>-0.495006</td>\n",
       "      <td>-1.034258</td>\n",
       "      <td>1.150955</td>\n",
       "      <td>-0.454578</td>\n",
       "      <td>-0.615178</td>\n",
       "      <td>-0.162992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.640306</td>\n",
       "      <td>1.261067</td>\n",
       "      <td>0.268524</td>\n",
       "      <td>-0.456021</td>\n",
       "      <td>0.850707</td>\n",
       "      <td>0.627662</td>\n",
       "      <td>-0.123396</td>\n",
       "      <td>-0.398559</td>\n",
       "      <td>0.055002</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.640306</td>\n",
       "      <td>1.380928</td>\n",
       "      <td>0.268524</td>\n",
       "      <td>-0.518751</td>\n",
       "      <td>0.300188</td>\n",
       "      <td>0.904649</td>\n",
       "      <td>-1.936895</td>\n",
       "      <td>0.352107</td>\n",
       "      <td>-0.207242</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2.362434</td>\n",
       "      <td>-0.596772</td>\n",
       "      <td>-0.630368</td>\n",
       "      <td>-0.399265</td>\n",
       "      <td>0.116682</td>\n",
       "      <td>1.181635</td>\n",
       "      <td>1.608414</td>\n",
       "      <td>0.934714</td>\n",
       "      <td>-0.158679</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.703025</td>\n",
       "      <td>1.320998</td>\n",
       "      <td>1.494287</td>\n",
       "      <td>0.915080</td>\n",
       "      <td>0.239019</td>\n",
       "      <td>0.443004</td>\n",
       "      <td>-0.254099</td>\n",
       "      <td>1.113977</td>\n",
       "      <td>0.793172</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-0.640306</td>\n",
       "      <td>0.781625</td>\n",
       "      <td>-0.393388</td>\n",
       "      <td>-0.966823</td>\n",
       "      <td>-1.840718</td>\n",
       "      <td>-1.772889</td>\n",
       "      <td>0.464766</td>\n",
       "      <td>-1.272468</td>\n",
       "      <td>-0.867710</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.798345</td>\n",
       "      <td>2.459674</td>\n",
       "      <td>2.556615</td>\n",
       "      <td>1.273538</td>\n",
       "      <td>1.707069</td>\n",
       "      <td>2.751226</td>\n",
       "      <td>-0.237761</td>\n",
       "      <td>-0.544210</td>\n",
       "      <td>-0.304370</td>\n",
       "      <td>-0.075846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1.098122</td>\n",
       "      <td>-0.956354</td>\n",
       "      <td>0.194979</td>\n",
       "      <td>1.034566</td>\n",
       "      <td>0.850707</td>\n",
       "      <td>0.627662</td>\n",
       "      <td>0.350401</td>\n",
       "      <td>0.654614</td>\n",
       "      <td>1.016566</td>\n",
       "      <td>-0.206564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Age    height    weight  Body Mass Index  Systolic blood pressure  \\\n",
       "0      0.861064 -0.357051 -0.050174         0.228036                -0.800849   \n",
       "1     -1.272461 -1.256006  0.309383         1.452767                -1.290199   \n",
       "2     -0.166189 -1.735448 -1.447544        -0.877208                -0.495006   \n",
       "3     -0.640306  1.261067  0.268524        -0.456021                 0.850707   \n",
       "4     -0.640306  1.380928  0.268524        -0.518751                 0.300188   \n",
       "...         ...       ...       ...              ...                      ...   \n",
       "99995  2.362434 -0.596772 -0.630368        -0.399265                 0.116682   \n",
       "99996  0.703025  1.320998  1.494287         0.915080                 0.239019   \n",
       "99997 -0.640306  0.781625 -0.393388        -0.966823                -1.840718   \n",
       "99998 -0.798345  2.459674  2.556615         1.273538                 1.707069   \n",
       "99999  1.098122 -0.956354  0.194979         1.034566                 0.850707   \n",
       "\n",
       "       diastolic blood pressure  Fasting plasma glucose  Cholesterol  \\\n",
       "0                     -0.572614               -0.139734    -0.969961   \n",
       "1                     -0.572614                0.334063    -0.656250   \n",
       "2                     -1.034258                1.150955    -0.454578   \n",
       "3                      0.627662               -0.123396    -0.398559   \n",
       "4                      0.904649               -1.936895     0.352107   \n",
       "...                         ...                     ...          ...   \n",
       "99995                  1.181635                1.608414     0.934714   \n",
       "99996                  0.443004               -0.254099     1.113977   \n",
       "99997                 -1.772889                0.464766    -1.272468   \n",
       "99998                  2.751226               -0.237761    -0.544210   \n",
       "99999                  0.627662                0.350401     0.654614   \n",
       "\n",
       "       Triglyceride  High-density lipoprotein cholesterol   ...  site_12  \\\n",
       "0          0.016151                              -0.860153  ...      0.0   \n",
       "1         -0.595753                              -0.075846  ...      0.0   \n",
       "2         -0.615178                              -0.162992  ...      0.0   \n",
       "3          0.055002                              -0.075846  ...      0.0   \n",
       "4         -0.207242                              -0.075846  ...      0.0   \n",
       "...             ...                                    ...  ...      ...   \n",
       "99995     -0.158679                              -0.075846  ...      0.0   \n",
       "99996      0.793172                              -0.075846  ...      0.0   \n",
       "99997     -0.867710                              -0.075846  ...      0.0   \n",
       "99998     -0.304370                              -0.075846  ...      0.0   \n",
       "99999      1.016566                              -0.206564  ...      0.0   \n",
       "\n",
       "       site_13  site_14  site_16  smoking status_1.0  smoking status_2.0  \\\n",
       "0          1.0      0.0      0.0                 0.0                 0.0   \n",
       "1          0.0      0.0      0.0                 0.0                 0.0   \n",
       "2          0.0      0.0      0.0                 0.0                 0.0   \n",
       "3          0.0      0.0      0.0                 0.0                 0.0   \n",
       "4          0.0      0.0      0.0                 0.0                 0.0   \n",
       "...        ...      ...      ...                 ...                 ...   \n",
       "99995      0.0      0.0      0.0                 0.0                 0.0   \n",
       "99996      0.0      0.0      0.0                 0.0                 0.0   \n",
       "99997      0.0      0.0      0.0                 0.0                 0.0   \n",
       "99998      0.0      0.0      0.0                 0.0                 0.0   \n",
       "99999      0.0      0.0      0.0                 0.0                 0.0   \n",
       "\n",
       "       smoking status_3.0  drinking status_1.0  drinking status_2.0  \\\n",
       "0                     1.0                  0.0                  0.0   \n",
       "1                     1.0                  0.0                  0.0   \n",
       "2                     1.0                  0.0                  0.0   \n",
       "3                     0.0                  0.0                  0.0   \n",
       "4                     1.0                  0.0                  0.0   \n",
       "...                   ...                  ...                  ...   \n",
       "99995                 0.0                  0.0                  0.0   \n",
       "99996                 0.0                  0.0                  0.0   \n",
       "99997                 0.0                  0.0                  0.0   \n",
       "99998                 0.0                  0.0                  0.0   \n",
       "99999                 0.0                  0.0                  0.0   \n",
       "\n",
       "       drinking status_3.0  \n",
       "0                      1.0  \n",
       "1                      1.0  \n",
       "2                      1.0  \n",
       "3                      0.0  \n",
       "4                      1.0  \n",
       "...                    ...  \n",
       "99995                  0.0  \n",
       "99996                  0.0  \n",
       "99997                  0.0  \n",
       "99998                  0.0  \n",
       "99999                  0.0  \n",
       "\n",
       "[100000 rows x 39 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['High-density lipoprotein cholesterol ', 'Low-density lipoprotein cholesterol', \n",
    "                     'Aspartate transaminase'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.loc[abs(X_train_scaled['High-density lipoprotein cholesterol ']) < 1e-15, 'High-density lipoprotein cholesterol '] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vRRxGAVcVuu"
   },
   "source": [
    "**Задание 2** Бэйзлайн – константное предсказание (0.5 балла).\n",
    "\n",
    "Как понять, работает ли та или иная модель, если сравить метрику не с чем? Чтобы было с чем сравнивать, соберём простой бэйзлайн: предскажем всем клиентам одну и ту же вероятность заболеть в течение 5 лет. Какое значение надо предсказать, чтобы минимизировать кросс-энтропию? Оцените качество такого предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss при константном предсказании:  0.09238070362646933\n"
     ]
    }
   ],
   "source": [
    "prob = y_train['diabettes in 5 years'].sum() / len(y_train) \n",
    "print(\"Log loss при константном предсказании: \", log_loss(y_train, np.full((len(y_train)), prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoAhwmWCQQRz"
   },
   "source": [
    "**Задание 3** Наивный байесовский классификатор (0.5 балла).\n",
    "\n",
    "Предположим, что в каждом из классе признаки независимы и имеют нормальное распределение. Тогда подобрать параметры этого распределения поможет модель [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). Обучите эту модель. Оцените её качество.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lab_train, X_lab_test, y_lab_train, y_lab_test = train_test_split(X_train, y_train, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "e-kO5ewdeOUQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_clf = GaussianNB()\n",
    "naive_bayes_clf.fit(X_lab_train, y_lab_train['diabettes in 5 years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наивный Бейес: качество на тестовой выборке:  2.0983496770164614\n"
     ]
    }
   ],
   "source": [
    "print(\"Наивный Бейес: качество на тестовой выборке: \", log_loss(y_lab_test['diabettes in 5 years'], naive_bayes_clf.predict_proba(X_lab_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age :   0.06323827122999537\n",
      "height :   0.061339113483283934\n",
      "weight :   0.06135826404925487\n",
      "Body Mass Index :   0.0612740777243939\n",
      "Systolic blood pressure :   0.06121220998110131\n",
      "diastolic blood pressure :   0.06120631092073917\n",
      "Fasting plasma glucose :   0.07650555602970316\n",
      "Cholesterol :   0.06121750597849511\n",
      "Triglyceride :   0.06125115153386845\n",
      "High-density lipoprotein cholesterol  :   0.06121440148658791\n",
      "Low-density lipoprotein cholesterol :   0.06121749174620527\n",
      "Alanine aminotransferase :   0.06154586673293504\n",
      "Aspartate transaminase :   0.06120757471677538\n",
      "Blood urea nitrogen :   0.06120794463399243\n",
      "Creatinine clearance rate :   0.06142089002864047\n",
      "family history of diabetes :   0.06129953650533578\n",
      "Gender_1 :   0.06120631067926907\n",
      "Gender_2 :   0.06120631067926907\n",
      "site_0 :   0.061206310679269064\n",
      "site_2 :   0.061206310679269064\n",
      "site_3 :   0.061206310679269064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-387-8ce674a9f2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     model = sm.GLM(y_train, X_train.drop(name, axis=1), \\\n\u001b[1;32m      3\u001b[0m                family=sm.families.Binomial(link=sm.genmod.families.links.logit()))\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":  \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, maxiter, method, tol, scale, cov_type, cov_kwds, use_t, full_output, disp, max_start_irls, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eim'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0mcov_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nonrobust'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             return self._fit_irls(start_params=start_params, maxiter=maxiter,\n\u001b[0m\u001b[1;32m   1064\u001b[0m                                   \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                                   cov_kwds=cov_kwds, use_t=use_t, **kwargs)\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36m_fit_irls\u001b[0;34m(self, start_params, maxiter, tol, scale, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_endog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                                             check_weights=True)\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mwls_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwls_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwls_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m             \u001b[0mlin_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwls_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0mlin_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset_exposure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/statsmodels/regression/_tools.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwendog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             params, _, _, _ = np.linalg.lstsq(self.wexog, self.wendog,\n\u001b[0m\u001b[1;32m    102\u001b[0m                                               rcond=-1)\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;31m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rhs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name in X_train.columns:\n",
    "    model = sm.GLM(y_train, X_train.drop(name, axis=1), \\\n",
    "               family=sm.families.Binomial(link=sm.genmod.families.links.logit()))\n",
    "    result = model.fit()\n",
    "    print(name, \":  \", log_loss(y_train, result.predict(X_train.drop(name, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.GLM(y_train, X_train, \\\n",
    "               family=sm.families.Binomial(link=sm.genmod.families.links.cauchy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07108742185790139"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_train, result.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best score: 0.061206310679269064 (0.6236) -> 0.06175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(replace_nan.fit_transform(y_test), columns=y_test.columns, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.get_dummies(y_test, columns=['Gender', 'site', 'smoking status', 'drinking status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>diastolic blood pressure</th>\n",
       "      <th>Fasting plasma glucose</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Triglyceride</th>\n",
       "      <th>High-density lipoprotein cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>site_12.0</th>\n",
       "      <th>site_13.0</th>\n",
       "      <th>site_14.0</th>\n",
       "      <th>site_16.0</th>\n",
       "      <th>smoking status_1.0</th>\n",
       "      <th>smoking status_2.0</th>\n",
       "      <th>smoking status_3.0</th>\n",
       "      <th>drinking status_1.0</th>\n",
       "      <th>drinking status_2.0</th>\n",
       "      <th>drinking status_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>25.9</td>\n",
       "      <td>139.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.33</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>105.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.64</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.76</td>\n",
       "      <td>5.37</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>173.5</td>\n",
       "      <td>69.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>111.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>4.82</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111828</th>\n",
       "      <td>40.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>131.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.62</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111829</th>\n",
       "      <td>30.0</td>\n",
       "      <td>152.8</td>\n",
       "      <td>45.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111830</th>\n",
       "      <td>53.0</td>\n",
       "      <td>174.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>29.2</td>\n",
       "      <td>123.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.74</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111831</th>\n",
       "      <td>32.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>119.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5.04</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111832</th>\n",
       "      <td>45.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>109.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.89</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111833 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age  height  weight  Body Mass Index  Systolic blood pressure  \\\n",
       "0       67.0   159.0    65.5             25.9                    139.0   \n",
       "1       27.0   177.0    80.0             25.5                    105.0   \n",
       "2       65.0   158.0    53.0             21.2                    108.0   \n",
       "3       30.0   173.5    69.0             22.9                    111.0   \n",
       "4       25.0   173.0    71.0             23.7                    112.0   \n",
       "...      ...     ...     ...              ...                      ...   \n",
       "111828  40.0   165.0    72.0             26.4                    131.0   \n",
       "111829  30.0   152.8    45.8             19.6                     97.0   \n",
       "111830  53.0   174.5    89.0             29.2                    123.0   \n",
       "111831  32.0   169.0    70.0             24.5                    119.0   \n",
       "111832  45.0   179.0    67.0             20.9                    109.0   \n",
       "\n",
       "        diastolic blood pressure  Fasting plasma glucose  Cholesterol  \\\n",
       "0                           65.0                    3.79         6.33   \n",
       "1                           62.0                    5.64         5.00   \n",
       "2                           67.0                    4.76         5.37   \n",
       "3                           76.0                    5.20         4.82   \n",
       "4                           67.0                    4.73         3.88   \n",
       "...                          ...                     ...          ...   \n",
       "111828                      88.0                    4.66         4.62   \n",
       "111829                      69.0                    4.50         3.29   \n",
       "111830                      75.0                    4.74         5.10   \n",
       "111831                      66.0                    5.04         4.09   \n",
       "111832                      73.0                    4.89         3.90   \n",
       "\n",
       "        Triglyceride  High-density lipoprotein cholesterol   ...  site_12.0  \\\n",
       "0               2.07                                   1.35  ...          0   \n",
       "1               1.40                                   1.35  ...          0   \n",
       "2               1.85                                   1.35  ...          0   \n",
       "3               1.06                                   1.40  ...          0   \n",
       "4               0.89                                   1.35  ...          0   \n",
       "...              ...                                    ...  ...        ...   \n",
       "111828          1.07                                   1.35  ...          0   \n",
       "111829          0.53                                   1.35  ...          0   \n",
       "111830          1.60                                   1.31  ...          0   \n",
       "111831          1.08                                   1.35  ...          0   \n",
       "111832          1.10                                   1.55  ...          0   \n",
       "\n",
       "        site_13.0  site_14.0  site_16.0  smoking status_1.0  \\\n",
       "0               0          0          0                   0   \n",
       "1               0          0          0                   0   \n",
       "2               0          0          0                   0   \n",
       "3               0          0          0                   0   \n",
       "4               0          0          0                   0   \n",
       "...           ...        ...        ...                 ...   \n",
       "111828          0          0          0                   0   \n",
       "111829          0          0          0                   0   \n",
       "111830          0          0          0                   0   \n",
       "111831          0          0          0                   0   \n",
       "111832          0          0          0                   0   \n",
       "\n",
       "        smoking status_2.0  smoking status_3.0  drinking status_1.0  \\\n",
       "0                        0                   1                    0   \n",
       "1                        0                   1                    0   \n",
       "2                        0                   1                    0   \n",
       "3                        0                   1                    0   \n",
       "4                        0                   1                    0   \n",
       "...                    ...                 ...                  ...   \n",
       "111828                   0                   1                    0   \n",
       "111829                   0                   1                    0   \n",
       "111830                   0                   1                    0   \n",
       "111831                   0                   1                    0   \n",
       "111832                   0                   1                    0   \n",
       "\n",
       "        drinking status_2.0  drinking status_3.0  \n",
       "0                         0                    1  \n",
       "1                         0                    1  \n",
       "2                         0                    1  \n",
       "3                         0                    1  \n",
       "4                         0                    1  \n",
       "...                     ...                  ...  \n",
       "111828                    0                    1  \n",
       "111829                    0                    1  \n",
       "111830                    0                    1  \n",
       "111831                    0                    1  \n",
       "111832                    0                    1  \n",
       "\n",
       "[111833 rows x 39 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled = y_test.copy()\n",
    "y_test_scaled.loc[:, :'Creatinine clearance rate'] = \\\n",
    "    scaler.transform(y_test.astype(np.double).loc[:, :'Creatinine clearance rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>diastolic blood pressure</th>\n",
       "      <th>Fasting plasma glucose</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Triglyceride</th>\n",
       "      <th>High-density lipoprotein cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>site_12</th>\n",
       "      <th>site_13</th>\n",
       "      <th>site_14</th>\n",
       "      <th>site_16</th>\n",
       "      <th>smoking status_1.0</th>\n",
       "      <th>smoking status_2.0</th>\n",
       "      <th>smoking status_3.0</th>\n",
       "      <th>drinking status_1.0</th>\n",
       "      <th>drinking status_2.0</th>\n",
       "      <th>drinking status_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.967336</td>\n",
       "      <td>-0.896424</td>\n",
       "      <td>0.064231</td>\n",
       "      <td>0.795594</td>\n",
       "      <td>1.217712</td>\n",
       "      <td>-0.849614</td>\n",
       "      <td>-1.838868</td>\n",
       "      <td>1.817803</td>\n",
       "      <td>0.710051</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.193442</td>\n",
       "      <td>1.261067</td>\n",
       "      <td>1.249135</td>\n",
       "      <td>0.676108</td>\n",
       "      <td>-0.862026</td>\n",
       "      <td>-1.126601</td>\n",
       "      <td>1.183631</td>\n",
       "      <td>0.327516</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.809298</td>\n",
       "      <td>-1.016284</td>\n",
       "      <td>-0.957239</td>\n",
       "      <td>-0.608365</td>\n",
       "      <td>-0.678519</td>\n",
       "      <td>-0.664956</td>\n",
       "      <td>-0.254099</td>\n",
       "      <td>0.742107</td>\n",
       "      <td>0.496207</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.956384</td>\n",
       "      <td>0.841555</td>\n",
       "      <td>0.350242</td>\n",
       "      <td>-0.100550</td>\n",
       "      <td>-0.495013</td>\n",
       "      <td>0.166004</td>\n",
       "      <td>0.464766</td>\n",
       "      <td>0.125823</td>\n",
       "      <td>-0.271687</td>\n",
       "      <td>0.125202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.351481</td>\n",
       "      <td>0.781625</td>\n",
       "      <td>0.513677</td>\n",
       "      <td>0.138422</td>\n",
       "      <td>-0.433844</td>\n",
       "      <td>-0.664956</td>\n",
       "      <td>-0.303112</td>\n",
       "      <td>-0.927462</td>\n",
       "      <td>-0.436930</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111828</th>\n",
       "      <td>-0.166189</td>\n",
       "      <td>-0.177260</td>\n",
       "      <td>0.595394</td>\n",
       "      <td>0.944952</td>\n",
       "      <td>0.728362</td>\n",
       "      <td>1.273952</td>\n",
       "      <td>-0.417477</td>\n",
       "      <td>-0.001587</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111829</th>\n",
       "      <td>-0.956384</td>\n",
       "      <td>-1.639560</td>\n",
       "      <td>-1.545605</td>\n",
       "      <td>-1.086309</td>\n",
       "      <td>-1.351376</td>\n",
       "      <td>-0.480298</td>\n",
       "      <td>-0.678882</td>\n",
       "      <td>-1.588566</td>\n",
       "      <td>-0.786856</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111830</th>\n",
       "      <td>0.861064</td>\n",
       "      <td>0.961416</td>\n",
       "      <td>1.984592</td>\n",
       "      <td>1.781353</td>\n",
       "      <td>0.239012</td>\n",
       "      <td>0.073675</td>\n",
       "      <td>-0.286774</td>\n",
       "      <td>0.439568</td>\n",
       "      <td>0.253203</td>\n",
       "      <td>-0.267857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111831</th>\n",
       "      <td>-0.798345</td>\n",
       "      <td>0.302182</td>\n",
       "      <td>0.431959</td>\n",
       "      <td>0.377394</td>\n",
       "      <td>-0.005663</td>\n",
       "      <td>-0.757285</td>\n",
       "      <td>0.203361</td>\n",
       "      <td>-0.692153</td>\n",
       "      <td>-0.252246</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111832</th>\n",
       "      <td>0.228908</td>\n",
       "      <td>1.500789</td>\n",
       "      <td>0.186807</td>\n",
       "      <td>-0.697980</td>\n",
       "      <td>-0.617351</td>\n",
       "      <td>-0.110983</td>\n",
       "      <td>-0.041707</td>\n",
       "      <td>-0.905051</td>\n",
       "      <td>-0.232806</td>\n",
       "      <td>0.780300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111833 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Age    height    weight  Body Mass Index  \\\n",
       "0       1.967336 -0.896424  0.064231         0.795594   \n",
       "1      -1.193442  1.261067  1.249135         0.676108   \n",
       "2       1.809298 -1.016284 -0.957239        -0.608365   \n",
       "3      -0.956384  0.841555  0.350242        -0.100550   \n",
       "4      -1.351481  0.781625  0.513677         0.138422   \n",
       "...          ...       ...       ...              ...   \n",
       "111828 -0.166189 -0.177260  0.595394         0.944952   \n",
       "111829 -0.956384 -1.639560 -1.545605        -1.086309   \n",
       "111830  0.861064  0.961416  1.984592         1.781353   \n",
       "111831 -0.798345  0.302182  0.431959         0.377394   \n",
       "111832  0.228908  1.500789  0.186807        -0.697980   \n",
       "\n",
       "        Systolic blood pressure  diastolic blood pressure  \\\n",
       "0                      1.217712                 -0.849614   \n",
       "1                     -0.862026                 -1.126601   \n",
       "2                     -0.678519                 -0.664956   \n",
       "3                     -0.495013                  0.166004   \n",
       "4                     -0.433844                 -0.664956   \n",
       "...                         ...                       ...   \n",
       "111828                 0.728362                  1.273952   \n",
       "111829                -1.351376                 -0.480298   \n",
       "111830                 0.239012                  0.073675   \n",
       "111831                -0.005663                 -0.757285   \n",
       "111832                -0.617351                 -0.110983   \n",
       "\n",
       "        Fasting plasma glucose  Cholesterol  Triglyceride  \\\n",
       "0                    -1.838868     1.817803      0.710051   \n",
       "1                     1.183631     0.327516      0.058800   \n",
       "2                    -0.254099     0.742107      0.496207   \n",
       "3                     0.464766     0.125823     -0.271687   \n",
       "4                    -0.303112    -0.927462     -0.436930   \n",
       "...                        ...          ...           ...   \n",
       "111828               -0.417477    -0.001587     -0.000777   \n",
       "111829               -0.678882    -1.588566     -0.786856   \n",
       "111830               -0.286774     0.439568      0.253203   \n",
       "111831                0.203361    -0.692153     -0.252246   \n",
       "111832               -0.041707    -0.905051     -0.232806   \n",
       "\n",
       "        High-density lipoprotein cholesterol   ...  site_12  site_13  site_14  \\\n",
       "0                                    0.005963  ...      0.0      0.0      0.0   \n",
       "1                                    0.005963  ...      0.0      0.0      0.0   \n",
       "2                                    0.005963  ...      0.0      0.0      0.0   \n",
       "3                                    0.125202  ...      0.0      0.0      0.0   \n",
       "4                                    0.005963  ...      0.0      0.0      0.0   \n",
       "...                                       ...  ...      ...      ...      ...   \n",
       "111828                               0.005963  ...      0.0      0.0      0.0   \n",
       "111829                               0.005963  ...      0.0      0.0      0.0   \n",
       "111830                              -0.267857  ...      0.0      0.0      0.0   \n",
       "111831                               0.005963  ...      0.0      0.0      0.0   \n",
       "111832                               0.780300  ...      0.0      0.0      0.0   \n",
       "\n",
       "        site_16  smoking status_1.0  smoking status_2.0  smoking status_3.0  \\\n",
       "0           0.0                 0.0                 0.0                 1.0   \n",
       "1           0.0                 0.0                 0.0                 0.0   \n",
       "2           0.0                 0.0                 0.0                 0.0   \n",
       "3           0.0                 0.0                 0.0                 0.0   \n",
       "4           0.0                 0.0                 0.0                 0.0   \n",
       "...         ...                 ...                 ...                 ...   \n",
       "111828      0.0                 0.0                 0.0                 0.0   \n",
       "111829      0.0                 0.0                 0.0                 0.0   \n",
       "111830      0.0                 0.0                 0.0                 1.0   \n",
       "111831      0.0                 0.0                 0.0                 1.0   \n",
       "111832      0.0                 0.0                 0.0                 0.0   \n",
       "\n",
       "        drinking status_1.0  drinking status_2.0  drinking status_3.0  \n",
       "0                       0.0                  0.0                  1.0  \n",
       "1                       0.0                  0.0                  0.0  \n",
       "2                       0.0                  0.0                  0.0  \n",
       "3                       0.0                  0.0                  0.0  \n",
       "4                       0.0                  0.0                  0.0  \n",
       "...                     ...                  ...                  ...  \n",
       "111828                  0.0                  0.0                  0.0  \n",
       "111829                  0.0                  0.0                  0.0  \n",
       "111830                  0.0                  0.0                  1.0  \n",
       "111831                  0.0                  0.0                  1.0  \n",
       "111832                  0.0                  0.0                  0.0  \n",
       "\n",
       "[111833 rows x 39 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = result.predict(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraim = pd.DataFrame()\n",
    "data_fraim['Id'] = y_test.index\n",
    "data_fraim['Prediction'] = rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraim.to_csv('ans5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bSiNwxJeO5u"
   },
   "source": [
    "**Задание 4** Дискриминантный анализ (1 балл).\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png)\n",
    "\n",
    "Теперь избавимся от предположения условной независимости признаков относительно целевой переменной. Таким образом, ковариационные матрицы распределений классов не обязательно будут даигональны. Мы можем наложить дополнительное условие в виде [равенства ковариационных матриц](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis) всех классов или [не делать этого](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis).\n",
    "\n",
    "Попробуйте оба варианта. Какой сработал лучше и чем это можно объяснить?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ykbQjaVNqEZa"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_clf = LinearDiscriminantAnalysis()\n",
    "linear_clf.fit(X_lab_train, y_lab_train['diabettes in 5 years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Квадратичный дискриминантный анализ: качество на тестовой выборке:  0.06421100219159141\n"
     ]
    }
   ],
   "source": [
    "print(\"Линейный дискриминантный анализ: качество на тестовой выборке: \", \\\n",
    "      log_loss(y_lab_test['diabettes in 5 years'], linear_clf.predict_proba(X_lab_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tronindmitr/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_clf = QuadraticDiscriminantAnalysis()\n",
    "quadratic_clf.fit(X_lab_train, y_lab_train['diabettes in 5 years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Квадратичный дискриминантный анализ: качество на тестовой выборке:  32.23057387487159\n"
     ]
    }
   ],
   "source": [
    "print(\"Квадратичный дискриминантный анализ: качество на тестовой выборке: \", \\\n",
    "      log_loss(y_lab_test['diabettes in 5 years'], quadratic_clf.predict_proba(X_lab_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за того что признаки линейно зависимы или почти линейно зависимы (например, масса тела и индекс массы тела) второй вариант не сходится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWwASoNGqE3k"
   },
   "source": [
    "**Задание 5** Логистическая регрессия (1 балл).\n",
    "\n",
    "Обучите модель логистической регрессии. Убедитесь, что модель сошлась. Удалось ли получить улучшение по сравнению с предыдущими моделями? Чем это можно объяснить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L652BU931R4D"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_clf = LogisticRegression(max_iter=5000, solver='liblinear')\n",
    "log_reg_clf.fit(X_lab_train, y_lab_train['diabettes in 5 years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: качество на тестовой выборке:  0.06085594682338117\n"
     ]
    }
   ],
   "source": [
    "print(\"Логистическая регрессия: качество на тестовой выборке: \", \\\n",
    "      log_loss(y_lab_test['diabettes in 5 years'], log_reg_clf.predict_proba(X_lab_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztZejuZ-1S68"
   },
   "source": [
    "**Задание 6** GLM – обобщённые линейные модели (1 балл).\n",
    "\n",
    "Как вы знаете, логистическая регрессия является частным случаем обобщённой линейной модели $\\mu(\\mathbb E(y|X)) = Xw$, где функция связи $\\mu(u) = \\log(\\frac{u}{1-u})$, и $y|X\\sim Bernoulli$.\n",
    "\n",
    "Учитывая, что целевая переменная бинарная, изменять класс распределений $y|X$ не имеет смысла. А вот изменить функцию связи можно.\n",
    "\n",
    "Обучите обобщённые линейные модели, в качестве функции связи использовав по крайней мере 2 разные функции, отличные от logit, использующейся в логистической регрессии.\n",
    "\n",
    "Реализацию GLM можно взять в пакете [statsmodels](https://www.statsmodels.org/stable/glm).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "BYyM7zftAaav"
   },
   "outputs": [],
   "source": [
    "glm_log_clf = sm.GLM(y_lab_train, X_lab_train, \\\n",
    "               family=sm.families.Binomial(link=sm.genmod.families.links.probit()))\n",
    "glm_log_clf_info = glm_log_clf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM cо связью probit: качество на тестовой выборке 0.061289207619131036\n"
     ]
    }
   ],
   "source": [
    "print(\"GLM cо связью probit: качество на тестовой выборке\", \\\n",
    "      log_loss(y_lab_test['diabettes in 5 years'], glm_log_clf_info.predict(X_lab_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_pow_clf = sm.GLM(y_lab_train, X_lab_train, \\\n",
    "               family=sm.families.Binomial(link=sm.genmod.families.links.cauchy()))\n",
    "glm_pow_clf_info = glm_pow_clf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM cо связью pow: качество на тестовой выборке 1.3152600173309557\n"
     ]
    }
   ],
   "source": [
    "print(\"GLM cо связью pow: качество на тестовой выборке\", \\\n",
    "      log_loss(y_lab_test['diabettes in 5 years'], glm_pow_clf_info.predict(X_lab_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsOjHdFuAa1B"
   },
   "source": [
    "В этой части работы мы построим теперь вероятностную модель, учитывающую специфику задачи.\n",
    "\n",
    "**Задание 7** Оценка Нельсона – Аалена (1 балл).\n",
    "\n",
    "Поскольку изначально все участники исследования были здоровы, заболеть, скажем, через месяц у них очень мало шансов. Скорее всего, это значило бы ошибку при проведении анализов. А вот заболеть через несколько лет шансов уже больше.\n",
    "\n",
    "Зависимость риска заболеть в данный момент времени $t$ при условии, что до момента $t$ человек оставался здоров, называется функцией риска ([hazard function](https://en.wikipedia.org/wiki/Survival_analysis#Hazard_function_and_cumulative_hazard_function)). В других задачах она позволяет определить, люди какого возраста наиболее подвержены заболеванию, или в какой момент эпидемии риск заразиться максимален.\n",
    "\n",
    "Оцените, какова вероятность заболеть через $t$ лет после начала исследования, воспользовавшись оценками Нельсона – Аалена:\n",
    "\n",
    "$$\\hat H_{NA}(t) = \\frac{d_t}{n_t},$$\n",
    "\n",
    "Где $n_t$ – количество участников, остававшихся здоровыми и не прекративших участие в исследовании до года $t$, $d_t$ – количество участников, заболевших в год $t.$\n",
    "\n",
    "Изобразите $\\hat H_{NA}(t)$ на графике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dWzhS3IOE0UE"
   },
   "outputs": [],
   "source": [
    "X_initial = pd.read_csv('X_train.csv')\n",
    "data = X_initial.join(y_train)\n",
    "data = data[~data['year of followup'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NA(data):\n",
    "    hazard_fun_values = np.empty((8))\n",
    "    for i in range(1, 9): \n",
    "        total = len(data[(data['year of followup'] > i) | ((data['year of followup'] > i - 1) & (data['year of followup'] <= i) & \n",
    "                       ((data['censor of diabetes at followup'] == 0)))])\n",
    "        ill = len(data[(data['year of followup'] > i - 1) & (data['year of followup'] <= i) & \n",
    "                       (data['censor of diabetes at followup'] == 1)])\n",
    "        print(total, ill)\n",
    "        if total == 0:\n",
    "            hazard_fun_values[i - 1] = 0\n",
    "        else:\n",
    "            hazard_fun_values[i - 1] = ill / total\n",
    "            \n",
    "    return hazard_fun_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.00866443, 0.01216124, 0.01961827,\n",
       "       0.03025794, 0.        , 0.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard_fun_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "sns.lineplot(y=hazard_fun_values, x=list(range(1, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pS7c-iScO1I"
   },
   "source": [
    "Оценка Нельсона – Аалена не персонализирована, т.е. для неё мы никак не использовали признаки пациентов.\n",
    "\n",
    "Модель пропорциональных рисков Кокса (Cox Proportional Hazard Model, CPHM) моделирует отношение риска для пациента к среднему риску по всем пациентам для данного момента времени. Поскольку это отношение рисков – положительное число, его обычно моделируют как $e^{Xw}$, где $X$ – признаки пациентов, $w$ – обучаемый вектор весов. Таким образом,\n",
    "\n",
    "$$\\hat H_{Cox}(x, t) = \\hat H_{NA}(t)e^{xw}.$$\n",
    "\n",
    "Обратите внимание, что в линейной части CPHM не используют свободный член: в некотором смысле его роль играет $\\hat H_{NA}(t)$.\n",
    "\n",
    "Вероятность заболеть ровно в момент $t$ – это произведение вероятности не заболеть до момента $t$ на вероятность заболеть в момент $t$ при условии здоровья до момента $t$. Таким образом, функция правдоподобия имеет вид\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\left[(\\hat H_{NA}(T_i)e^{x_iw})^{y_i}(1 - \\hat H_{NA}(T_i)e^{x_iw})^{1-y_i}\\prod_{t=0}^{T_i-1}\\left(1-\\hat H_{NA}(t)e^{x_iw}\\right)\\right],$$\n",
    "\n",
    "где $T_i$ – момент последней записи, $i$-го пациента.\n",
    "Максимизация правдоподобия эквивалентна минимизации «минус» нормированного логарифма правдоподобия.\n",
    "\n",
    "$$\\mathcal L = -\\frac{1}{N}\\log L = -\\frac{1}{N}\\sum_{i=1}^N\\left[y_i\\left(\\log \\hat H_{NA}(T_i) + x_iw\\right) + (1-y_i)\\left(\\log (1-\\hat H_{NA}(T_i) e^{x_iw})\\right) + \\sum_{t=0}^{T_i - 1}\\log(1-\\hat H_{NA}(t)e^{x_iw})\\right]$$\n",
    "\n",
    "$$\\nabla_w\\mathcal L = \\frac{1}{N}\\sum_{i=1}^N\\left[-x_iy_i + (1-y_i)\\frac{\\hat H_{NA}(T_i)e^{x_iw}x_i}{1-\\hat H_{NA}(T_i)e^{x_iw}} + \\sum_{t=0}^{T_i-1}\\frac{\\hat H_{NA}(t)e^{x_iw}x_i}{1-\\hat H_{NA}(t)e^{x_iw}}\\right]$$\n",
    "\n",
    "Минимизировать $\\mathcal L$ можно градиентным спуском, итеративно применяя формулу\n",
    "\n",
    "$$w:= w - \\eta \\nabla_w\\mathcal L - \\eta Cw$$\n",
    "\n",
    "где $\\eta > 0$ — размер шага (learning rate), $C\\geq 0$ — коэффициент регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEoag2gNBPe5"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBj__jTgHqBF"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**Задание 8** Cox Proportional Hazard Model (CPHM) (3 балла).\n",
    "\n",
    " Реализуйте градиентный спуск и примените его к модели CPHM.\n",
    "\n",
    "В качестве критерия останова мы предлагаем использовать следующие условия:\n",
    " - евклидова норма разности текущего и нового векторов весов стала меньше, чем 1e-4\n",
    " - ограничение на число итераций (например, 10000)\n",
    " \n",
    "Для начальной инициализации весов нужно сравнить следующие подходы:\n",
    " - нулевая начальная инициализация\n",
    " - случайная\n",
    " \n",
    "Выполните следующие пункты и прокомментируйте полученные результаты:\n",
    "- Рассмотрите как влияет размер шага на сходимость (попробуйте не менее 5-ти различных значений).\n",
    "- Рассмотрите регуляризованную модель (не менее 5-ти различных коэффициентов регуляризации), которая описана выше, а также модель без регуляризатора. Сравните, влияет ли наличие регуляризации на скорость сходимости и качество (под качеством во всех случаях подразумевается значение исходного, нерегуляризованного функционала).\n",
    "- Исследуйте качество оптимизируемого функционала в зависимости от номера итерации (при правильной реализации и подходящем размере шага он должен убывать).\n",
    "- Влияет ли выбор начальной инициализации весов на скорость и качество?\n",
    "\n",
    "В каждом пункте требуется построить необходимые графики скорости/качества и дать исчерпывающие выводы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_na = X_initial[['year of followup']].join(X_initial['censor of diabetes at followup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 0\n",
      "100000 0\n",
      "99141 859\n",
      "48597 591\n",
      "20695 406\n",
      "4032 122\n",
      "2 0\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "NA_matrix = NA(data_for_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_matrix = np.zeros((len(X_train_scaled), 8))\n",
    "H = np.empty(len(X_train_scaled))\n",
    "for i in range(len(X_train_scaled)):\n",
    "    T = int(np.floor(X_initial.loc[i, 'year of followup']))\n",
    "    H[i] = NA_matrix[T]\n",
    "    for t in range(2, T):\n",
    "        H_matrix[i, t] = NA_matrix[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def CPHMMatrix(X, y, H, H_matrix, times, step = 0.001, C=0, tol=None, max_iter=1000, weight_init = 'zero', verbose=False, \n",
    "               print_every=500):\n",
    "    if weight_init == 'zero':\n",
    "        coef = np.zeros(len(X[0]))\n",
    "    else:\n",
    "        coef = np.random.rand(len(X[0])) / 50\n",
    "        \n",
    "    print(coef)\n",
    "        \n",
    "    iteration = 0\n",
    "    \n",
    "    y = y[None].T\n",
    "        \n",
    "    H = H[None].T\n",
    "    \n",
    "    quality_in_iteration = []\n",
    "    iterations = []\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        e = np.exp((X @ coef)[None].T)\n",
    "    \n",
    "        H_na_e = H_matrix *  e        \n",
    "        \n",
    "        grad = (X * (-y + (1 - y) * H * e / \\\n",
    "                     (1 - H * e) + (H_na_e / \\\n",
    "                                    (1 - H_na_e)).sum(axis=1)[None].T)).mean(axis=0)        \n",
    "        \n",
    "        L = -(y * (np.log(np.clip(H, 1e-13, 1e13)) + (X @ coef)[None].T) + \\\n",
    "              (1 - y) * np.log(np.clip(1 - H * e, 1e-13, 1e13)) + \\\n",
    "              np.log(np.clip(1 - H_na_e, 1e-13, 1e13)).sum(axis=1)[None].T).mean(axis=0)\n",
    "        \n",
    "        \n",
    "        coef -= step * grad + step * C * coef\n",
    "        \n",
    "        if np.linalg.norm(step * grad + step * C * coef) < 1e-4:\n",
    "            if verbose:\n",
    "                print(\"End by norm\")\n",
    "            break\n",
    "        \n",
    "        iterations.append(iteration)\n",
    "        quality_in_iteration.append(L)\n",
    "        \n",
    "        if verbose and iteration % 500 == 0:\n",
    "            print(f\"Iteration {iteration}: \\t \\t loss {L}\")        \n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "    if verbose:\n",
    "            print(\"End by max_iter\")\n",
    "    return coef, quality_in_iteration, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00830537 0.01572289 0.01547227 0.01320454 0.01662408 0.00322918\n",
      " 0.01947311 0.01571362 0.00849629 0.00679359 0.01865376 0.01218195\n",
      " 0.00127917 0.00418511 0.0129339  0.0061729  0.01985694 0.00373918\n",
      " 0.01464861 0.000213   0.01266772 0.00838185 0.00898486 0.01755749\n",
      " 0.00750643 0.01126253 0.01158603 0.00346891 0.01322947 0.00185839\n",
      " 0.01158496 0.01620866 0.01908953 0.00938314 0.00294061 0.00817257\n",
      " 0.01601243 0.00525891 0.00105688]\n",
      "Iteration 0: \t \t loss [0.10492589]\n",
      "Iteration 500: \t \t loss [0.07965578]\n",
      "Iteration 1000: \t \t loss [0.07745895]\n",
      "Iteration 1500: \t \t loss [0.07653586]\n",
      "Iteration 2000: \t \t loss [0.07603532]\n",
      "Iteration 2500: \t \t loss [0.07572903]\n",
      "Iteration 3000: \t \t loss [0.07552716]\n",
      "Iteration 3500: \t \t loss [0.07538742]\n",
      "Iteration 4000: \t \t loss [0.07528701]\n",
      "Iteration 4500: \t \t loss [0.07521268]\n",
      "End by norm\n",
      "End by max_iter\n"
     ]
    }
   ],
   "source": [
    "weight, quality, iterations = CPHMMatrix(X_train_scaled.to_numpy(), \n",
    "                                   X_initial['censor of diabetes at followup'].to_numpy(), H, H_matrix, \n",
    "                                         X_initial['year of followup'], step=0.1, max_iter=10000, verbose=True, \n",
    "                                        weight_init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "X-svUu8CIX71"
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import time\n",
    "\n",
    "def CPHM(X, y, H, times, step = 0.001, C=0, tol=None, max_iter=1000, weight_init = 'zero'):\n",
    "    if weight_init == 'zero':\n",
    "        coef = np.zeros(len(X[0]))\n",
    "    else:\n",
    "        pass\n",
    "#         random generation on W\n",
    "    iteration = 0\n",
    "    err = np.inf\n",
    "\n",
    "    quality_in_iteration = []\n",
    "    iterations = []\n",
    "    \n",
    "    while (tol and abs(err) > tol) or (iteration < max_iter):\n",
    "        print(iteration)\n",
    "        start = time.time()\n",
    "        grad = np.zeros(len(X[0]))\n",
    "        function = 0\n",
    "\n",
    "        for i in range(0, len(X)):            \n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n",
    "            risk = X[i] @ coef\n",
    "            d_not_ill_before = 0.0\n",
    "            not_ill_before = 0.0            \n",
    "            \n",
    "            \n",
    "            for t in range(int(np.floor(times[i]))):\n",
    "                d_not_ill_before += H[t] / (np.exp(-risk) - H[t])\n",
    "                not_ill_before = np.log(np.clip(1 - H[t] * np.exp(risk), 0.001, None))\n",
    "                            \n",
    "            if y[i] == 0:\n",
    "                d_not_ill_before += H[int(np.floor(times[i]))] / (np.exp(-risk) - H[int(np.floor(times[i]))])\n",
    "                function += (np.log(np.clip(1 - H[int(np.floor(times[i]))] * np.exp(risk), 0.001, None)))\n",
    "                \n",
    "            d_not_ill_before -= y[i]\n",
    "            \n",
    "            grad += X[i] * d_not_ill_before\n",
    "            \n",
    "            function += y[i] * (np.log(np.clip(H[int(np.floor(times[i]))], 0.001, None)) + risk) + not_ill_before\n",
    "            \n",
    "\n",
    "            \n",
    "        grad /= len(X)\n",
    "        function = - function / len(X)\n",
    "        coef -= step * grad + step * C * coef\n",
    "\n",
    "        iterations.append(iteration)\n",
    "        quality_in_iteration.append(function)\n",
    "        print(f\"{iteration}: {function} {grad}\")\n",
    "        print (time.time() - start)\n",
    "        iteration += 1\n",
    "        \n",
    "    return coef, quality_in_iteration, iterations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.floor(X_initial['year of followup'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "0: 0.10465301627516024 [-1.93816793e-02 -9.53675608e-04 -1.45041012e-02 -1.81816041e-02\n",
      " -1.57182069e-02 -1.17005395e-02 -3.54770428e-02 -7.09245626e-03\n",
      " -1.48924155e-02  3.58678594e-03 -3.82756704e-03 -1.09910936e-02\n",
      " -5.86357873e-03 -5.70912880e-03 -3.67819195e-03 -2.14114945e-04\n",
      " -3.29013400e-03  3.59547006e-03  8.96975869e-06  1.39761551e-04\n",
      "  3.65940593e-04  1.28068452e-04 -9.54422087e-04  3.33580378e-05\n",
      "  3.79810785e-04  4.46099565e-04  3.08454088e-04 -7.62256278e-06\n",
      " -3.43546671e-04 -6.75634852e-05 -5.56813903e-05 -8.63417513e-05\n",
      "  1.00511793e-05 -7.97640428e-04 -1.12921979e-04  1.18976513e-03\n",
      " -1.33139173e-04  3.83002502e-05  3.74041649e-04]\n",
      "53.04338717460632\n",
      "1\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "1: 0.10167064268569542 [-1.84777941e-02 -4.67301661e-04 -1.32029799e-02 -1.68144018e-02\n",
      " -1.45292112e-02 -1.06207942e-02 -3.42735891e-02 -6.33336418e-03\n",
      " -1.38049786e-02  3.18493715e-03 -3.31160825e-03 -1.01123039e-02\n",
      " -5.35844138e-03 -5.14709035e-03 -2.98427828e-03 -2.17040389e-04\n",
      " -2.88907308e-03  3.14444696e-03  8.73706724e-06  5.01488524e-05\n",
      "  3.29714560e-04  1.33407856e-04 -8.94585015e-04  3.55570109e-05\n",
      "  3.90891772e-04  4.74594162e-04  2.63126517e-04 -9.06308732e-06\n",
      " -3.32847519e-04 -6.46197895e-05 -5.37937942e-05 -8.54078139e-05\n",
      "  9.51309754e-06 -7.40687468e-04 -1.03992878e-04  1.14304448e-03\n",
      " -1.23447082e-04  7.11142804e-05  3.50696937e-04]\n",
      "50.287251710891724\n",
      "2\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "2: 0.09901014477793828 [-1.75952235e-02 -3.69778331e-05 -1.19736232e-02 -1.55059498e-02\n",
      " -1.33843385e-02 -9.57994240e-03 -3.31122250e-02 -5.59727216e-03\n",
      " -1.26987972e-02  2.79721975e-03 -2.82779745e-03 -9.21683240e-03\n",
      " -4.79654264e-03 -4.61945787e-03 -2.35452909e-03 -2.15595625e-04\n",
      " -2.43294869e-03  2.80890175e-03  8.57681061e-06 -2.80872239e-06\n",
      "  3.13086399e-04  1.56412114e-04 -7.96625767e-04  3.99529257e-05\n",
      "  4.16929505e-04  5.16309254e-04  2.36613492e-04 -9.53118759e-06\n",
      " -3.18786432e-04 -6.06496245e-05 -4.89364215e-05 -8.36792392e-05\n",
      "  9.08995136e-06 -6.77744928e-04 -9.39219671e-05  1.13457299e-03\n",
      " -1.12727532e-04  1.08341947e-04  3.67291684e-04]\n",
      "47.647679567337036\n",
      "3\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-c2d91de1ff44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m weight, quality, iterations = CPHM(X_train_scaled.to_numpy(), \n\u001b[0m\u001b[1;32m      2\u001b[0m                                    \u001b[0mX_initial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'censor of diabetes at followup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNA_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_initial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year of followup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                    step=1, max_iter=10)\n",
      "\u001b[0;32m<ipython-input-71-91fd75d1774d>\u001b[0m in \u001b[0;36mCPHM\u001b[0;34m(X, y, H, times, step, C, tol, max_iter, weight_init)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0md_not_ill_before\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrisk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mnot_ill_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrisk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m     \"\"\"\n\u001b[0;32m-> 2103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_clip_dep_is_byte_swapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_clip_dep_is_byte_swapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_clip_dep_is_scalar_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# guarded to protect circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAS/SecondSemester/ML/ml_env/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weight, quality, iterations = CPHM(X_train_scaled.to_numpy(), \n",
    "                                   X_initial['censor of diabetes at followup'], NA_matrix, X_initial['year of followup'], \n",
    "                                   step=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.54108298e-01, -6.89453785e-03,  9.30125951e-02,  1.25293367e-01,\n",
       "        1.06952414e-01,  7.15872664e-02,  3.02158340e-01,  3.85978050e-02,\n",
       "        9.58483674e-02, -1.89239011e-02,  1.83309944e-02,  6.59670956e-02,\n",
       "        2.37587826e-02,  3.45889370e-02,  1.15014075e-02,  1.90727405e-03,\n",
       "        1.03513797e-02, -2.58041988e-02, -8.51940334e-05, -4.72508735e-04,\n",
       "       -3.64311580e-03, -3.06128569e-03,  3.56768563e-03, -6.36584017e-04,\n",
       "       -5.62099957e-03, -6.90869549e-03, -2.62509778e-03,  5.89544008e-05,\n",
       "        2.64398331e-03,  4.53946761e-04,  2.12368205e-04,  7.49724569e-04,\n",
       "       -8.60008830e-05,  4.95392334e-03,  6.42124820e-04, -1.30177775e-02,\n",
       "        7.94555541e-04, -2.19315849e-03, -6.02312641e-03])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD7CAYAAACWq8i5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzpklEQVR4nO3de0BUdf7/8efMgFeugwMOF0W8jly8hpFRaSi2QcPqEka2te7iz6zcrFRqv4G66Ybbttua1rbV7tqmGbVJILJEWql5ScMbg5cQRGG4CJKKJjLw+4N13MlVRgSHGd6Pv5g558x8zlvkNZ/znnOOorm5uRkhhBDCCkpbD0AIIYT9kNAQQghhNQkNIYQQVpPQEEIIYTUJDSGEEFaT0BBCCGE1CQ0hhBBWc7L1ADra6dP1NDXd+KkoXl4u1NSc64AR2SephyWpxxVSC0v2Xg+lUoGnZ+9rLrcqNIqLi0lOTqaurg4PDw/S0tIIDAy0WGfr1q28+uqrHDlyhEceeYSFCxeal5lMJl566SW2bNmCQqFg1qxZxMfHA7BixQrWrFmDt7c3AKNHjyY1NRWACxcu8Pzzz1NQUIBKpWLhwoVMmDDhhgrQ1NTcptC4vK24QuphSepxhdTCkiPXw6rQSE1NJTExEb1eT0ZGBikpKaxevdpinYCAAJYuXUpOTg4NDQ0WyzIzMyktLSU3N5e6ujri4uKIiIjA398fgLi4OIuQueydd97BxcWFzz77jJKSEh5++GFyc3Pp3fvaKSiEEKLjtNrTqKmpwWAwEBMTA0BMTAwGg4Ha2lqL9fr3749Op8PJ6eocys7OJj4+HqVSiVqtJioqipycnFYHt3HjRhISEgAIDAwkJCSEr776yqodE0II0f5aDQ2j0YiPjw8qlQoAlUqFt7c3RqPR6jcxGo34+vqaH2u1WioqKsyPN2zYQGxsLDNnziQ/P9/8fHl5OX5+ftfcTgghxK1l80b49OnTmT17Ns7Ozmzbto05c+aQnZ2Np6dnu7y+l5dLm7fVaFzbZQyOQuphSepxhdTCkiPXo9XQ0Gq1VFZWYjKZUKlUmEwmqqqq0Gq1Vr+JVqulvLycsLAwwHLmodFozOuNHz8erVbL0aNHCQ8Px9fXl7KyMtRqtXm7cePG3dAO1tSca1NTSqNxpbr67A1v56ikHpakHldILSzZez2USsV1P2y3enjKy8sLnU5HVlYWAFlZWeh0OvMfcmtMmTKF9PR0mpqaqK2tJS8vj+joaAAqKyvN6xUWFlJWVsaAAQPM261btw6AkpISDhw4QGRkpNXvK4QQon0prLmfRlFREcnJyZw5cwY3NzfS0tIICgoiKSmJuXPnEhoayu7du3nmmWc4d+4czc3NuLq6snTpUiIjIzGZTCxZsoRt27YBkJSUZG5wL1y4kIKCApRKJc7OzsydO5e7774bgPPnz5OcnExhYSFKpZL58+cTFRV1QzvYlpmGoaSW9/OO8v9ih9PPx3GnmTfC3j89tTepxxVSC0v2Xo/WZhpWhYY9a0tonD57kd+9/y0/XGxk/kOjCPBue1/EUdj7f4T2JvW4Qmphyd7rcdOHp7oiT9fuLHt8PM5OSn6/Np+TVfZ7dqcQQrQnCY1r0PbpzYLEUTipFCxfm8/JagkOIYSQ0LgOH89eLEwcjUql4Pdr8ymT4BBCdHESGq3wUfdiwUOjUCr/Exyn6m09JCGEsBkJDStovXqz4KFRKBQtwWGskeAQQnRNEhpW0nr1Zv5DowBYvkaCQwjRNUlo3ADfPi3B0dzczPK1+VTUnrf1kIQQ4paS0LhBfv8JjqamZpav+ZZKCQ4hRBciodEGfhoX5j80ikZTy4yj8rQEhxCia5DQaCN/jQsLHhrFpcYmlq/Jp0qCQwjRBUho3AR/bxeemz6Shksmlq/Np6rugq2HJIQQHUpC4yb183Fl/kOjuNhg4vdrvqVagkMI4cAkNNpBPx9Xnps+ih8aTCxfk88pCQ4hhIOS0Ggn/fu68uz0kVy42Mjytfmc+l6CQwjheCQ02lFgXzeenT6S+h8aWb4mn5rvf7D1kIQQol1JaLSzAVo3nps+kvofLrF87bfUnpHgEEI4DgmNDjBA68YzCSM5d+ESy9fkc/rsRVsPSQgh2oWERgcZ6OvOMw+O5Mz5Bpav+VaCQwjhECQ0OtBAP3eeSRhJXX0Dy9fKjEMIYf8kNDrYID93nnlwBHXnLvL7tfnUnZPgEELYL6tCo7i4mISEBKKjo0lISKCkpOSqdbZu3crUqVMJCQkhLS3NYpnJZGLx4sVERUUxadIk0tPTzctWrlzJ/fffT2xsLFOnTmXLli3mZcnJydx1113o9Xr0ej1vvPFGG3fTtgb7ezAvfgSnz7YEx/cSHEIIO+VkzUqpqakkJiai1+vJyMggJSWF1atXW6wTEBDA0qVLycnJoaGhwWJZZmYmpaWl5ObmUldXR1xcHBEREfj7+xMWFsbMmTPp2bMnhw4dYsaMGWzdupUePXoAMGvWLGbMmNFOu2s7QwI8mPfgCP744T6Wr81nQeJo3Ht3s/WwhBDihrQ606ipqcFgMBATEwNATEwMBoOB2tpai/X69++PTqfDyenqHMrOziY+Ph6lUolarSYqKoqcnBwAIiMj6dmzJwBDhw6lubmZurq6m92vTmlIgAdPx4dRc+YHfr82nzP1Da1vJIQQnUirMw2j0YiPjw8qlQoAlUqFt7c3RqMRtVpt1ZsYjUZ8fX3Nj7VaLRUVFVett379evr160ffvn3Nz/3tb39j3bp1BAQE8OyzzzJw4ECr3vMyLy+XG1r/v2k0rm3e9nqv6e7ei0Vv7+DV9H0se3w87i7d2/19OkJH1MOeST2ukFpYcuR6WHV46lbYtWsXr732Gu+++675uXnz5qHRaFAqlaxfv55f/epX5OXlmQPMGjU152hqar7h8Wg0rlRXn73h7azR1707v54Wymsf7Sf59S0899Ao3Hp17kNVHVkPeyT1uEJqYcne66FUKq77YbvVw1NarZbKykpMJhPQ0tSuqqpCq9VaPQitVkt5ebn5sdFotJhN5OfnM3/+fFauXElQUJD5eR8fH5TKliHGxcVx/vz5/zlDsUe6QDVzfxZG5ekLvLJ2L2fPy6EqIUTn12poeHl5odPpyMrKAiArKwudTmf1oSmAKVOmkJ6eTlNTE7W1teTl5REdHQ3A/v37mTdvHn/+858JDg622K6ystL885YtW1Aqlfj4+Fj9vp3dcHNwnOeVD/Zy7sIlWw9JCCGuS9Hc3NzqsZuioiKSk5M5c+YMbm5upKWlERQURFJSEnPnziU0NJTdu3fzzDPPcO7cOZqbm3F1dWXp0qVERkZiMplYsmQJ27ZtAyApKYmEhAQApk2bRllZmUUYLF++nKFDh/LYY49RU1ODQqHAxcWFBQsWMHLkyBvawc54eOrHDhbX8OePDuDr1YvnHhqFS0/nW/K+N8Lep9ztTepxhdTCkr3Xo7XDU1aFhj2zh9AAOHCshhUf78e3T2+em975gsPe/yO0N6nHFVILS/Zej5vuaYhbIzTIiyenhlJ+qp4/rNtL/Q9yqEoI0flIaHQiYQP78MRPQzlZdY4/fLCX8xIcQohORkKjkxkxqCU4TlSd4w/r9nL+h0ZbD0kIIcwkNDqhkYP7MOenIZRWnuOVD/I5I1/HFUJ0EhIandSowRqe+GkoZafq+d17e6iuk3uOCyFsT0KjExs5uA/PJozk7PlLLHtvD6WV9vuNDCGEY5DQ6OSGBHjw/IzRKJUKXn7/WwpLalvfSAghOoiEhh3w07jwm0fG4OXWg1c/3MeuwsrWNxJCiA4goWEn1G49SJ4xmiBfN97MKOCzb07YekhCiC5IQsOO9O7hzLMJIxk9RMPaz4+S/sV3OPgJ/UKITkZCw850c1YxJy6Ee0b5sXFHKe9sKKTR1GTrYQkhuohOcz8NYT2lUsEjk4fg4dKN9VuKOVPfwJyfhtCjm/xzCiE6lsw07JRCoeCB8QN47L5hFJTUttw+Vk4CFEJ0MAkNO3fXCF+enBrKyep6lr23hyo5CVAI0YEkNBzAqMEa5k8fRf2FlpMAj1fISYBCiI4hoeEgBvm78/yMMTipFKSt+ZYCOQlQCNEBJDQciG+f3vzmkbF4uffgTx/uY4fBMe6nLoToPCQ0HIyna3eef3g0A/3ceetTA7m7Sm09JCGEA5HQcEC9ejjzbMIIxgzR8MGm7/hw83c0yUmAQoh2YFVoFBcXk5CQQHR0NAkJCZSUlFy1ztatW5k6dSohISGkpaVZLDOZTCxevJioqCgmTZpEenr6TS8T1+fspOLxuBAmjPYjZ2cp72QZ5CRAIcRNs+pssNTUVBITE9Hr9WRkZJCSksLq1ast1gkICGDp0qXk5OTQ0GB5vkBmZialpaXk5uZSV1dHXFwcERER+Pv7t3mZaJ1SqWDGpCF4uHTnk6+Oceb8JZ6QkwCFEDeh1ZlGTU0NBoOBmJgYAGJiYjAYDNTWWn47p3///uh0Opycrv6DlJ2dTXx8PEqlErVaTVRUFDk5OTe1TFhHoVAQe0cgv7hvGIUlp1m+Jp8z9XISoBCibVoNDaPRiI+PDyqVCgCVSoW3tzdGo9HqNzEajfj6+pofa7VaKioqbmqZuDGRI3x5cloo5af+cxLg6fO2HpIQwg45/HEKLy+XNm+r0bi240hsb5LGlQBfd5a8vYOX388nNel2Bvl7WL29o9XjZkk9rpBaWHLkerQaGlqtlsrKSkwmEyqVCpPJRFVVFVqt1uo30Wq1lJeXExYWBljOINq6zFo1Nedoarrxbw5pNK5UVzvemdVevZxJfng0r67bR/LKrTz501CCB6hb3c5R69FWUo8rpBaW7L0eSqXiuh+2Wz085eXlhU6nIysrC4CsrCx0Oh1qdet/aC6bMmUK6enpNDU1UVtbS15eHtHR0Te1TLSd1qs3LzwyBo17D/6Uvo/tBXLITwhhHasOTy1atIjk5GRWrVqFm5ub+Su1SUlJzJ07l9DQUHbv3s0zzzzDuXPnaG5uZsOGDSxdupTIyEj0ej379u1j8uTJADzxxBMEBAQAtHmZuDmert1Jfng0Kz4+wF8zDZypbyA6vJ+thyWE6OQUzQ5+6zc5PHV9lxpN/DXTwO7D1USHBxA/YRBKheKq9bpKPawl9bhCamHJ3utx04enhGNzdlIxWx/CvaP9+feuE7ydKScBCiGuzeG/PSVap1QqSJw0GA/Xbnz85THOnm9gzk9D6dldfj2EEJZkpiGAlpMA748IZOZPdBQer2P5mny+l5MAhRA/IqEhLNwZpmXuz0Ix1taz7L3dVMpJgEKI/yKhIa4SNrAP8x8axYWLJpa9t4di4xlbD0kI0UlIaIj/aaCvO8/PGE13ZxXL1+Sz51ClrYckhOgEJDTENV0+CdDHsydL3t7Bxh3HcfBvaAshWiGhIa7Lw6U7z88Ywx1hvqR/UcRfPi3gYoPJ1sMSQtiIfKdStKp7NxULHhmL1rOAj74sovzUeZ6cFoq3R09bD00IcYvJTENYRaFQcN/t/Zn34AhOn/2B3/79GwqKa1vfUAjhUCQ0xA0JGeDFi4+OxdO1O69+uFf6HEJ0MRIa4oZ5e/biN4+MZcxQb+lzCNHFSGiINuneTcXj+mB+ds9AvimsYul7e6iuu2DrYQkhOpiEhmgzhULBT/7T56g98wNLpM8hhMOT0BA3LSTIi5THxuJxuc+xU/ocQjgqCQ3RLlr6HGNa+hybpc8hhKOS0BDtpkc3J+lzCOHgJDREu/qffY4S6XMI4SgkNESHsOhzrNtLzs5S6XMI4QAkNESHMfc5hmj4cPN3vJVp4OIl6XMIYc+suvZUcXExycnJ1NXV4eHhQVpaGoGBgRbrmEwmXnrpJbZs2YJCoWDWrFnEx8cDUF1dTUpKCidPnqSxsZHZs2ej1+sBWLBgAYcPHza/zuHDh1m5ciX33nsvK1asYM2aNXh7ewMwevRoUlNT22O/xS3So5sTj8eFkL3jOP/68hjlp+p5cmooGrlulRB2yarQSE1NJTExEb1eT0ZGBikpKaxevdpinczMTEpLS8nNzaWuro64uDgiIiLw9/fn5ZdfJiQkhDfeeIPa2lqmTp1KeHg4Wq2W5cuXm1/j0KFDPProo0RGRpqfi4uLY+HChe20u8IWLt9Ktp+PK3/JKGDJ379hdlwIwYFqWw9NCHGDWj08VVNTg8FgICYmBoCYmBgMBgO1tZbNzezsbOLj41EqlajVaqKiosjJyQFawuByEKjVaoYNG8bGjRuveq+PPvqI2NhYunXrdtM7Jjqf0CAvXpQ+hxB2rdXQMBqN+Pj4oFKpAFCpVHh7e2M0Gq9az9fX1/xYq9VSUVEBQHBwMNnZ2TQ3N3PixAny8/MpLy+32L6hoYHMzEymTZtm8fyGDRuIjY1l5syZ5Ofnt20vRafhI30OIezaLbmfRnJyMsuWLUOv1+Pr60tERIQ5hC7Ly8vD19cXnU5nfm769OnMnj0bZ2dntm3bxpw5c8jOzsbT09Pq9/bycmnzuDUa1zZv64jasx4pSRF8tOko720spKruAi88Fk5fr97t9vq3gvx+XCG1sOTI9Wg1NLRaLZWVlZhMJlQqFSaTiaqqKrRa7VXrlZeXExYWBljOPNRqNa+88op53aSkJAYNGmSx/ccff3zVLEOj0Zh/Hj9+PFqtlqNHjxIeHm71DtbUnKOp6cYPgWg0rlRXn73h7RxVR9TjnjAtXi7d+EtGAU+/+oVd9Tnk9+MKqYUle6+HUqm47oftVg9PeXl5odPpyMrKAiArKwudTodabfmfe8qUKaSnp9PU1ERtbS15eXlER0cDcPr0aRobGwHYvn07R44cMfdIACoqKtizZw+xsbEWr1lZWWn+ubCwkLKyMgYMGNDakIUdMfc5XKTPIYQ9sOrw1KJFi0hOTmbVqlW4ubmRlpYGtMwY5s6dS2hoKHq9nn379jF58mQAnnjiCQICAgDYv38/S5cuRalU4unpyZtvvknPnle+cvnJJ58wYcIE3N3dLd731VdfpaCgAKVSibOzM8uXL7eYfQjH4OPZi9/8fAzvbijkw83fcbzyLI/dN4zuzqrWNxZC3FKKZgf/WCeHp9rHrahHc3Oz+XyOAG8XnpwaSp9Oej6H/H5cIbWwZO/1uOnDU0LcKpfP5/h1/AhOff8DS/6xG4Nct0qITkVCQ3Q6YQNb+hzuvbvxh3V7+fcu6XMI0VlIaIhO6XKfY8wQDes2fcdf5XwOIToFCQ3RaV2+btW0u4PYaajkd+/toUruzyGETUloiE7tx32ORe/u4uuDRjlcJYSNSGgIuxA20ItFM2+jn48rb2cV8pdPCzj/wyVbD0uILkdCQ9iNPu49WfDQKKbdHcSew9WkvLuLw6WnbT0sIboUCQ1hV5TKlsNVLzwyBmeVkuVr8vnoiyIaTU22HpoQXYKEhrBLA7RuLPpFOJEjfMnecZyl7+3BWFNv62EJ4fAkNITd6t5NxWP3DePJqaHUfP8Di//+DV/sLZMmuRAdSEJD2L3RQzQsnhnOYD93Vucc5vV/HeDM+QZbD0sIhyShIRyCp2t35iWMZPq9gzlwrIbUd3Zx8FiNrYclhMOR0BAOQ6lQMPm2AF589DZcejrz6of7WJN3hEuNcia5EO1FQkM4nABvF158dCxRY/zJ232SJf/Yzcmqc7YelhAOQUJDOKRuzioSJw1h3oMjOHv+Ekv+sZvPvjlBkzTJhbgpEhrCoYUGebHkl+GEDFCz9vOj/PHDfdSdu2jrYQlhtyQ0hMNz69WNp6aF8vPooRw9UUfKO7vIP1Jt62EJYZckNESXoFAouGeUH6m/uA0vtx6s+NcB/pFziIsN0iQX4kZIaIguRevVm9/8fAz33d6Pr/aWs+jv31BsPGPrYQlhN6wKjeLiYhISEoiOjiYhIYGSkpKr1jGZTCxevJioqCgmTZpEenq6eVl1dTWPP/44sbGx3HfffWRkZJiXrVixgoiICPR6PXq9nsWLF5uXXbhwgaeffppJkyYxZcoUNm/efBO7KkQLJ5WS+HsGMf+hUTRcMrHsvT1s2F7SpnvJC9HVOFmzUmpqKomJiej1ejIyMkhJSWH16tUW62RmZlJaWkpubi51dXXExcURERGBv78/L7/8MiEhIbzxxhvU1tYydepUwsPD0Wq1AMTFxbFw4cKr3vedd97BxcWFzz77jJKSEh5++GFyc3Pp3bt3O+y66OqG9fdkyS/D+UfOYT7+8hgHjtWSFDMcL/ceth6aEJ1WqzONmpoaDAYDMTExAMTExGAwGKitrbVYLzs7m/j4eJRKJWq1mqioKHJycgA4dOgQkZGRAKjVaoYNG8bGjRtbHdzGjRtJSEgAIDAwkJCQEL766qsb20MhrqN3D2ce1wfzy/t1HK88S8q7u9hpqLT1sITotFoNDaPRiI+PDyqVCgCVSoW3tzdGo/Gq9Xx9fc2PtVotFRUVAAQHB5OdnU1zczMnTpwgPz+f8vJy87obNmwgNjaWmTNnkp+fb36+vLwcPz+///maQrQXhULB+FAti39xG75evfjLpwX8NdPAhYuNth6aEJ2OVYenblZycjLLli1Dr9fj6+tLRESEOYSmT5/O7NmzcXZ2Ztu2bcyZM4fs7Gw8PT3b5b29vFzavK1G49ouY3AUjl4PjcaVPzyt4cO8I3zw2WGKjGd4NnE0wwd4XXN90UJqYcmR69FqaGi1WiorKzGZTKhUKkwmE1VVVeZ+xH+vV15eTlhYGGA581Cr1bzyyivmdZOSkhg0aBAAGo3G/Pz48ePRarUcPXqU8PBwfH19KSsrQ61Wm19z3LhxN7SDNTXn2tTg1Ghcqa4+e8PbOaquVI+o0X4E+rjw1qcFJK/cSkxEILHjA3FSXZmYd6V6tEZqYcne66FUKq77YbvVw1NeXl7odDqysrIAyMrKQqfTmf+QXzZlyhTS09NpamqitraWvLw8oqOjATh9+jSNjS1T/e3bt3PkyBFzj6Sy8srx48LCQsrKyhgwYID5NdetWwdASUkJBw4cMPdGhOhIg/zcWTwznDuC+5L5dQkvv/8tlafP23pYQticotmKO9YUFRWRnJzMmTNncHNzIy0tjaCgIJKSkpg7dy6hoaGYTCaWLFnCtm3bgJbZxOUm9pdffsnSpUtRKpV4enqSkpKCTqcDYOHChRQUFKBUKnF2dmbu3LncfffdAJw/f57k5GQKCwtRKpXMnz+fqKioG9pBmWm0j65cj12FlazOOYypqZnESYO5M1SLt7dbl63Hj3Xl343/xd7r0dpMw6rQsGcSGu2jq9ej9swPvJ1l4FBpHWOHang6cQwNF+RGTyC/Gz9m7/W46cNTQghQu/XguemjiL9nIPlHT/F42uds2V8ut5YVXY6EhhBWUioV3Hd7fxb94jb8vV35W/Yh0tbkY6ypt/XQhLhlJDSEuEF+GhdefuJOHrtvGCerzpHyzi7WbzkmdwgUXcItOU9DCEejVCq4a4QvIwb1Yd2mo3y6rYSdhkp+Hj0UXaC69RcQwk7JTEOIm+DeuxuzYoN5NmEkzc3w+w/28naWgTPnpUkuHJOEhhDtIHiAmiW/DCfmjv7sNFTym7d2SKNcOCQJDSHaSTdnFVPvGsiiX9yGb5/e5kZ5+SlplAvHIaEhRDvz07iw8OHR5kZ56ru7+OQraZQLxyCNcCE6gFJh2SjP/LqEXYXSKBf2T2YaQnSg/9Uo/2umNMqF/ZLQEOIW+O9G+a7C/zTK90mjXNgfCQ0hbpGrGuUbpVEu7I+EhhC3mDTKhT2TRrgQNnCtRvkj0UMZLo1y0YnJTEMIG/pxo/wVaZSLTk5CQ4hOQBrlwl5IaAjRSUijXNgDCQ0hOpn/bpSXVUujXHQu0ggXohO63CgfOagPH0ijXHQiMtMQohNzk0a56GSsmmkUFxeTnJxMXV0dHh4epKWlERgYaLGOyWTipZdeYsuWLSgUCmbNmkV8fDwA1dXVpKSkcPLkSRobG5k9ezZ6vR6AlStXkp2djVKpxNnZmXnz5hEZGQlAcnIyX3/9NZ6engBMmTKFxx9/vL32XQi7cblRnrX9OBt3HGd/0SniJwzizjAtSoXC1sMTXYhVoZGamkpiYiJ6vZ6MjAxSUlJYvXq1xTqZmZmUlpaSm5tLXV0dcXFxRERE4O/vz8svv0xISAhvvPEGtbW1TJ06lfDwcLRaLWFhYcycOZOePXty6NAhZsyYwdatW+nRowcAs2bNYsaMGe2/50LYmZZGeRDjhvuwOucQf994iE17ThI/YRDBA+SQlbg1Wj08VVNTg8FgICYmBoCYmBgMBgO1tbUW62VnZxMfH49SqUStVhMVFUVOTg4Ahw4dMs8e1Go1w4YNY+PGjQBERkbSs2dPAIYOHUpzczN1dXXttoNCOBq/Pr1Z+PBoZsUO5/zFRv6wbi9/WLeX0sqzth6a6AJaDQ2j0YiPjw8qlQoAlUqFt7c3RqPxqvV8fX3Nj7VaLRUVFQAEBweTnZ1Nc3MzJ06cID8/n/Ly8qvea/369fTr14++ffuan/vb3/5GbGwsc+bMoaioqG17KYSDUSoU3B7cl6VJtzN94iBKjGdY/LdveDvLQO2ZH2w9POHAbsm3p5KTk1m2bBl6vR5fX18iIiLMIXTZrl27eO2113j33XfNz82bNw+NRoNSqWT9+vX86le/Ii8v76ptr8fLy6XN49ZoXNu8rSOSeljqLPV4+H539BMG89Gmo3y65RjfHKrigcggfnbvEFx6Ot+SMXSWWnQWjlyPVkNDq9VSWVmJyWRCpVJhMpmoqqpCq9VetV55eTlhYWGA5cxDrVbzyiuvmNdNSkpi0KBB5sf5+fnMnz+fVatWERQUZH7ex8fH/HNcXBy/+93vqKiowM/Pz+odrKk5R1PTjZ9Vq9G4Ul0t0/3LpB6WOmM97h/Xj9uHefPJlmP8a/N35GwvIXb8ACaM8sPZqeO+KNkZa2FL9l4PpVJx3Q/brf4meXl5odPpyMrKAiArKwudTodabdl4mzJlCunp6TQ1NVFbW0teXh7R0dEAnD59msbGRgC2b9/OkSNHzD2S/fv3M2/ePP785z8THBxs8ZqVlZXmn7ds2YJSqbQIEiGEJS/3HvwqZjgpj91G/76ufPD5UX7z1x3sNFTSJJckEe1A0WzFxW2KiopITk7mzJkzuLm5kZaWRlBQEElJScydO5fQ0FBMJhNLlixh27ZtQMtsIiEhAYAvv/ySpUuXolQq8fT0JCUlBZ1OB8C0adMoKyuzCIPly5czdOhQHnvsMWpqalAoFLi4uLBgwQJGjhx5QzsoM432IfWwZC/1OFhcw4ebijhZfY7Avq48OGEQw/p7tut72EstbhV7r0drMw2rQsOeSWi0D6mHJXuqR1NTM9sLKvhkyzFqz1wkbKAX8fcMxE/T9n7ff7OnWtwK9l6P1kJDLiMihINTKhWMD9Vy2zBvPt9zkqztx0l5dxd3hmqJiwzC07W7rYco7IiEhhBdRDdnFffd3p/IEb5kfV3C53tOstNQyeTwAO4b15+e3eXPgWid/JYI0cW49HRm+r2DmTjGn0++OkbW18f5cm85D4wfwN0jfXFSySXpxLXJb4cQXZS3R0/+3wPBvPjoWPz69Ob9z47w4ts72X2oSm7+JK5JQkOILm6A1o35D43i6fgwnFRKVq0/yLL39nDkRJ2thyY6ITk8JYRAoVAQNrAPIQO82HrAyPotx3j5/W8ZNbgPP7tnIFqv3rYeougkJDSEEGZKZcvNn8bpfMjdfYKNO47z4tu7uGukL/rxgbi7yDetujoJDSHEVbp3UxF7RyB3j/Alc1sJX+wtY/vBCqaM60d0eAA9usmfjq5K/uWFENfk1rsbD08eQtRYfz7+soiMrcVszi8j7s4BRI7QolJKW7SrkdAQQrTKR92LOT8N5buy7/lw83es/vdhPtt9gp/dM5BJfdrnzHJhH+RjghDCaoP83Hn+4dE8NTWU5mZY8fEBFr6+lQPHauRrul2EzDSEEDdEoVAwaoiGsEFefLXPyMYdx/njh/vo7+PK/RH9GT1UI/ctd2ASGkKINlEplUwY5cdPJw4h84ujZO84zqr1B9F69eInt/dn3HAfObvcAUloCCFuirOTksgRvowP1bL7cBVZXx/nnQ2FrN9SzH239+POUC3dnK2/26bo3CQ0hBDtQqlUEK7z4bZh3uwvqiFrewn/zD3Cp9tKiL4tgHtG+clFER2A/AsKIdqVQqFgxKA+hA304siJOrK2Hyf9iyI2bD/OvWP8iRrrj2uvbrYepmgjCQ0hRIdQKBQM7efJ0H6eFBvPkL39OJlfl/Dvb0q5Z6Qf0eH95F4edkhCQwjR4QZo3Xhiaihlp+rJ3n6cvN0n2fTtScaHarlvXD+8PXvZeojCShIaQohbxq9Pb5JihxMXOYCcnaVs2W/kq33ljNP58JOI/vi30y1oRceR0BBC3HIaj548Ej2U2PGB5H5zgs35ZewwVDJqcB/ujwgkyNfN1kMU12DVl6iLi4tJSEggOjqahIQESkpKrlrHZDKxePFioqKimDRpEunp6eZl1dXVPP7448TGxnLfffeRkZFh1XbXWyaEsH8eLt15cMIgfv/4HejvHMCRE3W8tHo3v1+bT2FJrZxl3glZNdNITU0lMTERvV5PRkYGKSkprF692mKdzMxMSktLyc3Npa6ujri4OCIiIvD39+fll18mJCSEN954g9raWqZOnUp4eDharfa6211vmRDCcbj0dEZ/5wAm3xbAl3vL+fc3pfz+g70E+bpxf0R/RgzqI2eZdxKtzjRqamowGAzExMQAEBMTg8FgoLa21mK97Oxs4uPjUSqVqNVqoqKiyMnJAeDQoUNERkYCoFarGTZsGBs3bmx1u+stE0I4np7dnZgyrh/LZ0fw8+ihnKlvYMXHB1j07i52GCowNTXZeohdXqszDaPRiI+PDypVyxmdKpUKb29vjEYjarXaYj1fX1/zY61WS0VFBQDBwcFkZ2cTGhrKyZMnyc/PN88Wrrfd9ZZZy8ur7Y01jca1zds6IqmHJanHFR1Ri3itB1PvHcKWvWV8+PlR3vrUQOa240ybOIiJYwNwduq8Z5k78u/GLWmEJycns2zZMvR6Pb6+vkRERJhDqKPV1JyjqenGj4tqNK5UV5/tgBHZJ6mHJanHFR1di+B+HqQ+Npa9R0+xYXsJr6fv458bC5kS3o+7R/rRvVvnCg97/91QKhXX/bDdamhotVoqKysxmUyoVCpMJhNVVVVotdqr1isvLycsLAywnCWo1WpeeeUV87pJSUkMGjSo1e2ut0wI0XUoFQpGD9EwanAfDMdPs+HrEj7Y9B1Z248zaaw/E8f407uHs62H2SW02tPw8vJCp9ORlZUFQFZWFjqdzuLQFMCUKVNIT0+nqamJ2tpa8vLyiI6OBuD06dM0NjYCsH37do4cOWLukVxvu+stE0J0PQqFguBANQsSR/PCI2MY5OfOJ1uKmb/qa9ZtOkpV3QVbD9HhWXV4atGiRSQnJ7Nq1Src3NxIS0sDWmYMc+fOJTQ0FL1ez759+5g8eTIATzzxBAEBAQDs37+fpUuXolQq8fT05M0336Rnz54A193uesuEEF3bID935v4sjBNV58je0XKWee6uE4QEeTFxtB+hQV4olfKNq/amaHbwL0JLT6N9SD0sST2u6Cy1OH32Il/tK+eLvWV8f66BPu49uGeUH5Fh2lt6gcTOUo+2uumehhBC2ANP1+7o7xzA/RH92Xv0FJu+PclHXxSxfksxtw3zZuIYP4K0bijkfI+bIqEhhHAoTiolY4d5M3aYN2Wn6tn87Um+PljB9oIK+vu4MnG0H+HDfeguN4ZqEzk8dQ32PsVsb1IPS1KPK+yhFhcuNrLDUMmmb09SVl1P7x5OjA/VMmGUHz7q9r3Crj3U43rk8JQQosvr2d2JCaP8uGekL0dPfs+mb0/y+Z6T5H5zgpABaiaM9mPEwD7SOLeChIYQostQKBQMCfBgSIAHdedaGudf7i1nxccH8HLr/p/GuS9uveXOgtcioSGE6JI8XLrzwPj/bpyX8fGXx8jYWszYYd5MHO3PQF9pnP+YhIYQoktTKZWMGerNmKHeGGvq2fxtGdsOGtlRUEk/bxcmjvFnnM6n012uxFakEX4N9t7Mam9SD0tSjyscsRY/NDSyo6ClcX6yup6e3Z0YH9qXiaP96dtK49ze6yGNcCGEuEE9ujlxzyg/7v5P43xzfhmbvy0jb/dJhgd6MnG0PyMGeaFSWnUfO4cioSGEENfw343z6fcObjnjPL+M1/91ALVbd+4e6cddI3xx70KNcwkNIYSwgnvvbsTeEchPbu/Hvu9q2PztST756hif/qdxPmGUH4P93W09zA4noSGEEDdApVQyeoiG0UM0GGvq+SK/nK0HjOw0VOKvceEn4wcwvJ87brfwele3kjTCr8Hem1ntTephSepxhdQCLjaY2FnY0jgvrTyHSqkgZICaiJC+jBrcp1PfZfDHpBEuhBAdrHs3FXeN8OWuEb7UNzazYUsROwoq2FdUQ8/uKsYO9eaOkL4MDvBAaefnfUhoCCFEOwrUuvHghEH87O6BFJaeZvvBCnYVVrFlvxEvtx7cHuzDHSF90Xr1tvVQ20RCQwghOoBS2XKXweBANY9MNvHt0Wq2H6wge8dxNmw/TmBfVyJC+jJO52NXly2R0BBCiA7WvZuKiOC+RAT3pe7cRXYaKtl+sIK1eUdZ9/l3hASpuSOkLyMH9aFbJ79ku4SGEELcQh4u3YkO70d0eD9OVp9je0EFOwoqeTOjgJ7dVYwZ6k1EcF+G9uuc/Q8JDSGEsBF/jQvx9wxi2l0DOVx6mq8LKvjmUBVb9xtRu3Xn9uF9iQjpi1+fztP/sCo0iouLSU5Opq6uDg8PD9LS0ggMDLRYx2Qy8dJLL7FlyxYUCgWzZs0iPj4egJqaGp5//nmMRiONjY2MGzeO//u//8PJyYkFCxZw+PBh8+scPnyYlStXcu+997JixQrWrFmDt7c3AKNHjyY1NbWddl0IIToHpVKBLlCNLlDNjMkm8o9Ws/1gJTk7S8necZz+Pv/pfwz3sfnZ51aFRmpqKomJiej1ejIyMkhJSWH16tUW62RmZlJaWkpubi51dXXExcURERGBv78/b775JgMHDuStt97i0qVLJCYmkpuby09+8hOWL19ufo1Dhw7x6KOPEhkZaX4uLi6OhQsXttPuCiFE59bdWcXtw/ty+/C+fF/fYO5/fPD5UT7c9B3BA9REhPgwarDGJresbfVqWzU1NRgMBmJiYgCIiYnBYDBQW1trsV52djbx8fEolUrUajVRUVHk5OQALddvqa+vp6mpiYaGBi5duoSPj89V7/XRRx8RGxtLt272800CIYToKO69uzH5tgBSf3Ebv/3VOKaM60fZqXO89amBeSu28s4GA4UltTTdwnO0W51pGI1GfHx8UKlaEk2lUuHt7Y3RaEStVlus5+vra36s1WqpqKgAYM6cOTz11FPceeedXLhwgYcffpgxY8ZYvE9DQwOZmZn8/e9/t3h+w4YNbN26FY1Gw1NPPcWoUaPavLNCCGGv/Pr05mf3DGTq3UEcLq1j+8EKdh+uYtuBCjxdu7ec/xHcFz/Ntc/mbg+3pBGek5PD0KFD+cc//kF9fT1JSUnk5OQwZcoU8zp5eXn4+vqi0+nMz02fPp3Zs2fj7OzMtm3bmDNnDtnZ2Xh6elr93tc7Hb41Go1rm7d1RFIPS1KPK6QWljq6Hj7ebtw1th8/NDTyTUElm/ac4N+7TrBxRylBfu5MHBtAzPgBqFTtf+n2VkNDq9VSWVmJyWRCpVJhMpmoqqpCq9VetV55eTlhYWGA5czjn//8J8uWLUOpVOLq6srEiRPZuXOnRWh8/PHHTJs2zeI1NRqN+efx48ej1Wo5evQo4eHhVu+gXHuqfUg9LEk9rpBaWLrV9Rjm78Yw/2DO1A9mZ2FL/+PtjIO493AieIC69Rf4kdauPdVqDHl5eaHT6cjKygIgKysLnU5ncWgKYMqUKaSnp9PU1ERtbS15eXlER0cD4O/vz1dffQW0HIbavn07gwcPNm9bUVHBnj17iI2NtXjNyspK88+FhYWUlZUxYMCA1oYshBBdjlvvbkwaG0DKY7fx+tORDA+0/ojMjbDq8NSiRYtITk5m1apVuLm5kZaWBkBSUhJz584lNDQUvV7Pvn37mDx5MgBPPPEEAQEBALzwwgukpqYSGxuLyWRi3LhxPPjgg+bX/+STT5gwYQLu7pbXon/11VcpKChAqVTi7OzM8uXLLWYfQgghrtarh3OHvbZcGv0aZMptSephSepxhdTCkr3X46YPTwkhhBCXSWgIIYSwmoSGEEIIq0loCCGEsJqEhhBCCKs5/KXRlcq2X4/+ZrZ1RFIPS1KPK6QWluy5Hq2N3eG/ciuEEKL9yOEpIYQQVpPQEEIIYTUJDSGEEFaT0BBCCGE1CQ0hhBBWk9AQQghhNQkNIYQQVpPQEEIIYTUJDSGEEFaT0PgfiouLSUhIIDo6moSEBEpKSmw9JJs4ffo0SUlJREdHExsby5NPPkltba2th9UpvP766wwdOpQjR47Yeig2c/HiRVJTU5k8eTKxsbG8+OKLth6STW3evJm4uDj0ej0PPPAAubm5th5Sh5DLiPwPP//5z5k2bRp6vZ6MjAw+/vhjVq9ebeth3XJ1dXUcPnyYcePGAZCWlsb333/PsmXLbDwy2yooKOCPf/wjx44d480332TIkCG2HpJNvPTSSyiVSp5//nkUCgWnTp2iT58+th6WTTQ3NxMeHs7777/PkCFDOHToEA899BB79uxBqXSsz+aOtTftoKamBoPBQExMDAAxMTEYDIYu+Qnbw8PDHBgAI0eOpLy83IYjsr2GhgaWLFnCokWLbD0Um6qvr2f9+vX8+te/RqFoucBdVw2My5RKJWfPttzm9ezZs3h7eztcYEAXuMrtjTIajfj4+KBSqQBQqVR4e3tjNBpRq9U2Hp3tNDU1sXbtWiZOnGjrodjUa6+9xgMPPIC/v7+th2JTJ06cwMPDg9dff52dO3fSu3dvfv3rXzN27FhbD80mFAoFf/rTn5gzZw69evWivr6et956y9bD6hCOF4OiQ/z2t7+lV69ezJgxw9ZDsZn8/HwOHjxIYmKirYdicyaTiRMnTjB8+HD+9a9/8dxzz/HUU09x7tw5Ww/NJhobG/nLX/7CqlWr2Lx5M2+88QZPP/009fX1th5au5PQ+BGtVktlZSUmkwlo+c9RVVWFVqu18chsJy0tjePHj/OnP/3JIafb1vrmm28oKiri3nvvZeLEiVRUVPDLX/6SrVu32npot5xWq8XJycl8GHfEiBF4enpSXFxs45HZRmFhIVVVVYwZMwaAMWPG0LNnT4qKimw8svbXdf8CXIOXlxc6nY6srCwAsrKy0Ol0XfbQ1KuvvsrBgwdZuXIl3bp1s/VwbGrWrFls3bqVTZs2sWnTJvr27cs777zDnXfeaeuh3XJqtZpx48axbds2oOUbhzU1NfTv39/GI7ONvn37UlFRwbFjxwAoKiqipqaGfv362Xhk7U++PfU/FBUVkZyczJkzZ3BzcyMtLY2goCBbD+uWO3r0KDExMQQGBtKjRw8A/P39WblypY1H1jlMnDixS3976sSJE7zwwgvU1dXh5OTE008/zd13323rYdnMp59+yl//+lfzFwPmzp1LVFSUjUfV/iQ0hBBCWE0OTwkhhLCahIYQQgirSWgIIYSwmoSGEEIIq0loCCGEsJqEhhBCCKtJaAghhLCahIYQQgir/X9tKITeZx3UHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "sns.lineplot(x=iterations, y=quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2JKTJ3TbMVX"
   },
   "source": [
    "Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому обычно используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит сразу по этому объекту.\n",
    "\n",
    "**Задание 9** SGD (1 балл)\n",
    "\n",
    "Реализуйте метод стохастического градиентного спуска (sgd). В этом случае вы можете выбрать наиболее удачный функционал, исходя из предыдущего пункта (с регуляризацией, без), а также схему начальной инициализации весов.\n",
    "\n",
    "Сравните рассмотренные методы (градиентный спуск и sgd) между собой с точки зрения скорости сходимости и качества.\n",
    "\n",
    "Посмотрите как влияет размер шага на сходимость (попробуйте 4-5 различных значений)\n",
    "Исследуйте качество оптимизируемого функционала в зависимости от номера итерации\n",
    "Выберите лучший размер шага.\n",
    "\n",
    "В каждом пункте сделайте исчерпывающие выводы, подкреплённые графиками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGq9ukpHbOyr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m9nbbgybPCW"
   },
   "source": [
    "Между обновлением вектора весов по всей выборке и на одном объекте есть промежуточный подход — выбирать некоторое случайное подмножество объектов и обновлять веса по нему. Такой подход называется mini-batch. Мы не будем реализовывать этот подход в данной работе, однако иногда его бывает осмысленно использовать на практике. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdOsnrm6bPjs"
   },
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окрестности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{w}{J(w)}$$\n",
    "$$ w = w - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $w$ — вектор параметров\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n",
    " \n",
    "### Adagrad \n",
    "\n",
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное отличие данного метода от SGD состоит в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженным данным большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $w_i$ на итерации $t$ как $g_{t,i} = \\nabla_{w}J(w_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $w_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ w_{t+1, i} = w_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ w_{t+1, i} = w_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $w_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ w_{t+1} = w_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta, в отличии от Adagrad, рассматривает не все предыдущие значения градиентов, а только последние $k$. Кроме того, сумма градиентов определяется как уменьшающееся среднеее всех предыдущих квадратов градиентов. Текущее среднее $E[g^2]_t$ на итерации $t$ будет вглядеть следующим образом:\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\n",
    "\n",
    "здесь $\\gamma$ аналогична гиперпараметру из метода Momentum.\n",
    "\n",
    "Тогда обновление весов можно записать следующим образом:\n",
    "\n",
    "$$ w_{t+1} = w_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Перепишем это немного по-другому:\n",
    "\n",
    "$$ w_{t+1} = w_{t} + \\Delta w_t$$ \n",
    "$$\\Delta w_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Аналогично среднему для градиентов определим среднее для параметров $w$:\n",
    "\n",
    "$$ E[\\Delta w^2]_t = \\gamma E[\\Delta w^2]_{t-1} + (1-\\gamma)\\Delta w^2 $$\n",
    "\n",
    "Введем обозначение $RMS[p]_t = \\sqrt{E[p]_t + \\varepsilon}$\n",
    "\n",
    "Тогда Adadelta выглядит следующим образом:\n",
    "\n",
    "$$\\Delta w_t = - \\dfrac{RMS[\\Delta w^2]}{RMS[ga^2]} g_t $$ \n",
    "$$ w_{t+1} = w_{t} + \\Delta w_t$$ \n",
    "\n",
    "\n",
    "Более подробно об этих и других способах оптимизации можно прочитать:\n",
    " - [здесь](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) очень хорошее описание различных способов оптимизации, в этом задании мы опираемся на терминологию из данной статьи\n",
    " - статья про [momentum](https://pdfs.semanticscholar.org/97da/c94ffd7a7ac09a4218848300cc7e98569d77.pdf)\n",
    " - оригинальная статья про [adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    " - оригинальная статья про [adadelta](http://arxiv.org/pdf/1212.5701v1.pdf)\n",
    " - википедия про [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) и [adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)\n",
    " - [визуализация](http://imgur.com/a/Hqolp) разных способов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAb2ou0FgL8K"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Cun8h8db_F9"
   },
   "source": [
    "**Задание 10** Реализуйте метод оптимизации *Momentum* (0.5 балла) и \n",
    "\n",
    "**Задание 11** Реализуйте один из *Adagrad*/*Adadelta* (1 балл).\n",
    "\n",
    "В заданиях 10, 11:\n",
    "- Сравните оба метода с классическим sgd с точки зрения скорости сходимости.\n",
    "- Посмотрите как значение гиперпараметра $\\gamma$ влияет на скорость сходимости и качество в методе *Momentum*.\n",
    "\n",
    "Постройте графики и опишите полученные результаты.\n",
    "\n",
    "Дало ли преимущество использование адаптивного шага в методе *Adagrad*/*Adadelta*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz8ZzohGhOUm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh172idwhSBU"
   },
   "source": [
    "**Задание 12** Предсказание с помощью CPHM, интерфейс sklearn (1 балл)\n",
    "\n",
    "\n",
    "Чтобы сравнить CPHM с предыдущими моделями, надо научиться предсказывать целевую переменную. С помощью значений персональной функции риска $\\hat H_{Cox}(x, t)$ можно выразить вероятность того, что человек в течение 5 лет не заболеет при условии, что он не прекратит участие в эксперементе по другим причинам. Вероятность того, что человек не прекратит участие в эксперименте, оставаясь здоровым, также можно оценить из данных.\n",
    "\n",
    "Реализуйте модель CPHM, унаследовав класс модели от [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator). Модель должна поддерживать методы fit, predict, predict_proba. Это вам позволит в следующем задании её откалибровать. При тестировании модели следует передать то время, через которое вас интересует состояние пациента (в случае конкурса, 5 лет).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvYwCN55gO8j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xtdNig2NDoQ"
   },
   "source": [
    "\n",
    "**Задание 13** Калибровка классификатора (2 балла)\n",
    "\n",
    "Сравнительно простой способ уточнить оценки вероятности и уменьшить logloss -- откалибровать модель. Вообще говоря, сравнивать между собой некалиброванные модели по logloss некорректно: хорошая модель с нарушенной калибровкой может иметь сколь угодно большое значение logloss.\n",
    "\n",
    "Идея калибровки состоит в том, чтобы подобрать простое преобразование, которое превратит выходы моделей в вероятности принадлежности классам.\n",
    "\n",
    "Есть несколько известных методов калибровки:\n",
    " - Калибровка Платта.\n",
    " - Изотоническая регрессия.\n",
    "\n",
    "Обратите внимание, что преобразование необходимо обучать на отложенной выборке (то есть классификатор и калибровка должны обучаться на разных подмножествах данных), иначе можно переобучиться. Калибровку можно применять к любым классификаторам (где это разумно и необходимо), особенно к тем, которые не оптимизируют logloss явно.\n",
    "\n",
    "Для калибровки классификатора в sklearn возможны два подхода:\n",
    " - взять уже обученный классификатор и откалибровать его на отложенной выборке\n",
    " - откалибровать по кросс-валидации: калибровочному классификатору передается вся обучающая выборка, которая внутри разбивается на обучающую и калибровочную, после чего происходит усреднение вероятностей по фолдам.\n",
    " \n",
    "Подробнее об этом можно прочитать в [документации](http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV). Также [здесь](https://jmetzen.github.io/2015-04-14/calibration.html) можно узнать подробности о калибровке в sklearn от автора.\n",
    "\n",
    "Используйте оба описанных выше подхода (калибровку Платта и изотоническую регрессию) для калибровки моделей, построенных в лабораторной работе. Для каждой модели постройте график, на котором будут изображены [калибровочные кривые](http://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html): идеальная, исходного классификатора, а также для каждого из методов. Калибровочная кривая строится путем упорядочения всех объектов по предсказанному значению, которые разбиваются на бины. По оси OX откладывается среднее предсказанное значение вероятности по бину, а по OY — доля положительных примеров. В случае идеальных вероятностей это будет прямая.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jnNocIJofF5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYR8QBLooeWY"
   },
   "source": [
    "Выполнив задания, не забудьте отправить решение в [конкурс](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv8F9EcNNEVg"
   },
   "source": [
    "**Выводы** (0.5 балла)\n",
    "\n",
    "- Какие есть достоинства и недостатки у рассмотренных в лабораторной работе моделей?\n",
    "- Какие модели сильно улучшили свой logloss после калибровки, а какие - нет? Почему так произошло?\n",
    "- Какие из рассмотренных методов могут обучиться на маленьких датасетах? А какие применимы для очень больших датасетов? Какие проблемы могут возникнуть при применении к большим датасетам и как их можно решить?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wUMGH8PnHLG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab2_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
